{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "6f52424e-f841-4ec9-a83a-9e1fe3e06f1e",
      "cell_type": "markdown",
      "source": "# Trabajo #1: Análisis de Face Tracking y Visión por Computadora\n**Proyecto de Métodos Numéricos**\n\n### 1. ¿Qué es MindAR?\nMindAR es una biblioteca de software de código abierto y ligera diseñada para desarrollar experiencias de Realidad Aumentada (AR) en la web. Permite el reconocimiento de imágenes y seguimiento facial directamente en el navegador utilizando tecnologías estándar como WebGL y WebAssembly, eliminando la necesidad de instalar aplicaciones externas.\n\n### 2. ¿Qué es OpenCV?\nOpenCV (Open Source Computer Vision Library) es la biblioteca de visión artificial de código abierto más utilizada a nivel mundial. Provee una infraestructura común para aplicaciones de visión por computadora y contiene más de 2500 algoritmos optimizados para tareas como detección de rostros, identificación de objetos, clasificación de acciones en video, rastreo de movimientos y procesamiento de imágenes (filtros, bordes, transformaciones).\n\n### 3. ¿De manera interna MindAR usa OpenCV?\n**No.** Aunque ambas herramientas procesan imágenes, MindAR no depende de OpenCV.\n* **MindAR** está construida sobre TensorFlow.js y utiliza modelos de aprendizaje profundo (Deep Learning) propietarios y ligeros para realizar la detección y seguimiento de características faciales o imágenes planas.\n* **OpenCV** se basa en algoritmos clásicos de procesamiento de matrices de píxeles, mientras que MindAR se basa en inferencia de redes neuronales.\n\n### 4. ¿Se puede utilizar OpenCV en JavaScript?\n**Sí.** Existe una versión oficial llamada OpenCV.js. Mediante la tecnología WebAssembly (Wasm), el código original de C++ de OpenCV es compilado para que pueda ser ejecutado directamente por el navegador web (lado del cliente) con un rendimiento cercano al nativo, permitiendo realizar procesamiento de imágenes complejo en tiempo real dentro de páginas web.\n\n### 5. ¿Para qué sirve el algoritmo de Canny Edge Detection?\nEl algoritmo de Canny es una técnica de procesamiento de imágenes utilizada para detectar bordes de manera robusta. Es considerado el algoritmo estándar óptimo para esta tarea porque cumple tres criterios clave:\n1.  **Detección:** Baja tasa de error (encuentra todos los bordes reales).\n2.  **Localización:** Los puntos detectados deben estar lo más cerca posible del borde real.\n3.  **Respuesta única:** Debe marcar una sola línea por cada borde (evita bordes gruesos o múltiples respuestas al mismo contorno).\n\n### 6. Ejemplo del algoritmo de Canny Edge Detection en JavaScript\n\n# Análisis Matemático: Implementación \"Desde Cero\" de Canny Edge\n\nEn esta versión final, nuestro equipo decidió no depender de librerías externas para el procesamiento central. Hemos programado el algoritmo de Canny manualmente, manipulando los arrays de píxeles (`Uint8ClampedArray`) directamente. Esto nos permitió aplicar las ecuaciones matemáticas de visión artificial de forma explícita en cada etapa.\n\n### 1. Pre-procesamiento: Luminancia (Espacio Vectorial)\nLa primera operación matemática es reducir la dimensionalidad de la imagen de $\\mathbb{R}^3$ (RGB) a $\\mathbb{R}^1$ (Grises). No usamos un promedio simple; aplicamos la ecuación de **Luminancia Percibida** que pondera los canales según la sensibilidad del ojo humano:\n\n$$\nI(x,y) = 0.299 \\cdot R + 0.587 \\cdot G + 0.114 \\cdot B\n$$\n\n> **En nuestro código:** Iteramos sobre el buffer de datos crudos (`imageData.data`) aplicando esta combinación lineal punto a punto.\n\n---\n\n### 2. Suavizado Gaussiano (Convolución Discreta)\nPara eliminar el ruido de alta frecuencia, aplicamos una operación de convolución con un Kernel Gaussiano de $5 \\times 5$.\n\nMatemáticamente, el valor de cada píxel suavizado $S(x,y)$ es la suma ponderada de sus vecinos, definida por la matriz de convolución $K$:\n\n$$\nS(x,y) = \\frac{1}{159} \\sum_{i=-2}^{2} \\sum_{j=-2}^{2} I(x+i, y+j) \\cdot K(i,j)\n$$\n\nDonde $159$ es el factor de normalización (la suma de todos los elementos del kernel) para mantener el brillo original.\n\n---\n\n### 3. Cálculo de Gradientes (Diferenciación Numérica)\nImplementamos manualmente los operadores de Sobel para calcular las derivadas parciales. Para cada píxel, calculamos:\n* **Magnitud del Gradiente ($|G|$):** La hipotenusa del vector gradiente.\n  $$|G| = \\sqrt{G_x^2 + G_y^2}$$\n* **Dirección del Gradiente ($\\theta$):** El ángulo de orientación del borde.\n  $$\\theta = \\arctan\\left(\\frac{G_y}{G_x}\\right)$$\n\n> **En nuestro código:** Usamos `Math.sqrt` y `Math.atan2` sobre los resultados de las máscaras de convolución vertical y horizontal.\n\n---\n\n### 4. Supresión de No-Máximos (Discretización de Ángulos)\nPara adelgazar los bordes a 1 píxel de ancho, realizamos una comparación direccional. Como los píxeles están en una rejilla cuadrada, discretizamos el ángulo continuo $\\theta$ en 4 sectores principales:\n* **Horizontal:** $0^\\circ$\n* **Vertical:** $90^\\circ$\n* **Diagonal Principal:** $45^\\circ$\n* **Diagonal Invertida:** $135^\\circ$\n\nLa lógica matemática aplicada es:\n$$\nNMS(x,y) = \\begin{cases} \n|G(x,y)| & \\text{si } |G(x,y)| \\ge |G(vecinos_{\\theta})| \\\\\n0 & \\text{en caso contrario}\n\\end{cases}\n$$\nEsto elimina cualquier píxel que no sea el \"pico\" local de intensidad en la dirección del borde.\n\n---\n\n### 5. Histéresis y Conectividad (Lógica Topológica)\nFinalmente, clasificamos los bordes usando dos umbrales escalares ($T_{high} = 90$ y $T_{low} = 30$):\n1.  **Bordes Fuertes ($> T_{high}$):** Se aceptan incondicionalmente ($255$).\n2.  **Bordes Débiles ($T_{low} < x < T_{high}$):** Se marcan temporalmente ($128$).\n3.  **Ruido ($< T_{low}$):** Se descartan ($0$).\n\nPara resolver los \"Bordes Débiles\", aplicamos un análisis de vecindad de 8-conexión. Un borde débil sobrevive **solo si** existe al menos un vecino fuerte en su vecindad inmediata ($3 \\times 3$), garantizando la continuidad topológica de los contornos detectados.",
      "metadata": {}
    },
    {
      "id": "cacbbd02-d432-4795-b79e-0bbcdca61cc0",
      "cell_type": "code",
      "source": "%%html\n<div style=\"text-align: center; background: #1a1a1a; padding: 20px; border-radius: 15px; color: white; font-family: sans-serif;\">\n    <h3>Método Canny Edge(Derivadas y Bordes)</h3>\n    <video id=\"v_canny_pro\" width=\"640\" height=\"480\" style=\"display:none\" playsinline></video>\n    <canvas id=\"c_canny_pro\" width=\"640\" height=\"480\" style=\"background: #000; border: 2px solid #27ae60; border-radius: 10px;\"></canvas>\n    <div style=\"margin-top: 15px;\">\n        <button id=\"btn_on_canny\" onclick=\"iniciarCannyPro()\" style=\"padding: 10px 20px; background: #27ae60; color: white; border: none; border-radius: 5px; cursor: pointer; font-weight: bold;\">ACTIVAR CANNY</button>\n        <button id=\"btn_off_canny\" onclick=\"detenerTodo()\" style=\"padding: 10px 20px; background: #e74c3c; color: white; border: none; border-radius: 5px; cursor: pointer; margin-left: 10px; display: none;\">APAGAR</button>\n    </div>\n    <p id=\"log_canny_pro\" style=\"color: #f1c40f; font-size: 13px; margin-top: 10px;\">Estado: Esperando cámara...</p>\n</div>\n\n<script>\n// SISTEMA GLOBAL DE GESTIÓN DE CÁMARA\nif (typeof window.cameraManager === 'undefined') {\n    window.cameraManager = {\n        currentStream: null,\n        currentFilter: null,\n        activeLoop: null\n    };\n}\n\nfunction detenerTodo() {\n    if (window.cameraManager.activeLoop) {\n        window.cameraManager.activeLoop = false;\n    }\n    \n    if (window.cameraManager.currentStream) {\n        window.cameraManager.currentStream.getTracks().forEach(t => t.stop());\n        window.cameraManager.currentStream = null;\n    }\n    \n    // LIMPIAR TODOS LOS CANVAS\n    ['c_canny_pro', 'c_out', 'c_sobel_pro'].forEach(canvasId => {\n        const canvas = document.getElementById(canvasId);\n        if (canvas) {\n            const ctx = canvas.getContext('2d');\n            ctx.clearRect(0, 0, canvas.width, canvas.height);\n            ctx.fillStyle = '#000';\n            ctx.fillRect(0, 0, canvas.width, canvas.height);\n        }\n    });\n    \n    ['btn_off_canny', 'btn_off_sobel', 'b_stop'].forEach(id => {\n        const btn = document.getElementById(id);\n        if (btn) btn.style.display = 'none';\n    });\n    \n    ['btn_on_canny', 'btn_on_sobel', 'b_start'].forEach(id => {\n        const btn = document.getElementById(id);\n        if (btn) btn.style.display = 'inline-block';\n    });\n    \n    ['log_canny_pro', 'log_sobel_pro', 'debug_log'].forEach(id => {\n        const log = document.getElementById(id);\n        if (log) log.innerText = \"Cámara liberada.\";\n    });\n    \n    window.cameraManager.currentFilter = null;\n}\n\n// Implementación REAL de Canny Edge Detection\nfunction applyCannyEdgeDetection(imageData) {\n    const width = imageData.width;\n    const height = imageData.height;\n    const data = imageData.data;\n    \n    // 1. Convertir a escala de grises\n    const gray = new Uint8ClampedArray(width * height);\n    for (let i = 0; i < data.length; i += 4) {\n        const idx = i / 4;\n        gray[idx] = 0.299 * data[i] + 0.587 * data[i + 1] + 0.114 * data[i + 2];\n    }\n    \n    // 2. Suavizado Gaussiano 5x5\n    const smoothed = new Uint8ClampedArray(width * height);\n    const gaussianKernel = [\n        2, 4, 5, 4, 2,\n        4, 9, 12, 9, 4,\n        5, 12, 15, 12, 5,\n        4, 9, 12, 9, 4,\n        2, 4, 5, 4, 2\n    ];\n    const kernelSum = 159;\n    \n    for (let y = 2; y < height - 2; y++) {\n        for (let x = 2; x < width - 2; x++) {\n            let sum = 0;\n            for (let ky = -2; ky <= 2; ky++) {\n                for (let kx = -2; kx <= 2; kx++) {\n                    const idx = (y + ky) * width + (x + kx);\n                    sum += gray[idx] * gaussianKernel[(ky + 2) * 5 + (kx + 2)];\n                }\n            }\n            smoothed[y * width + x] = sum / kernelSum;\n        }\n    }\n    \n    // 3. Calcular gradientes con Sobel\n    const gradX = new Float32Array(width * height);\n    const gradY = new Float32Array(width * height);\n    const magnitude = new Float32Array(width * height);\n    const direction = new Float32Array(width * height);\n    \n    for (let y = 1; y < height - 1; y++) {\n        for (let x = 1; x < width - 1; x++) {\n            const idx = y * width + x;\n            \n            // Sobel X\n            const gx = (\n                -smoothed[(y-1)*width + (x-1)] + smoothed[(y-1)*width + (x+1)] +\n                -2*smoothed[y*width + (x-1)] + 2*smoothed[y*width + (x+1)] +\n                -smoothed[(y+1)*width + (x-1)] + smoothed[(y+1)*width + (x+1)]\n            );\n            \n            // Sobel Y\n            const gy = (\n                -smoothed[(y-1)*width + (x-1)] - 2*smoothed[(y-1)*width + x] - smoothed[(y-1)*width + (x+1)] +\n                smoothed[(y+1)*width + (x-1)] + 2*smoothed[(y+1)*width + x] + smoothed[(y+1)*width + (x+1)]\n            );\n            \n            gradX[idx] = gx;\n            gradY[idx] = gy;\n            magnitude[idx] = Math.sqrt(gx * gx + gy * gy);\n            direction[idx] = Math.atan2(gy, gx);\n        }\n    }\n    \n    // 4. Supresión no-máxima (esto hace que Canny sea diferente de Sobel)\n    const suppressed = new Float32Array(width * height);\n    \n    for (let y = 1; y < height - 1; y++) {\n        for (let x = 1; x < width - 1; x++) {\n            const idx = y * width + x;\n            const angle = direction[idx] * 180 / Math.PI;\n            const mag = magnitude[idx];\n            \n            let n1 = 0, n2 = 0;\n            \n            // Determinar vecinos según la dirección del gradiente\n            if ((angle >= -22.5 && angle < 22.5) || (angle >= 157.5 || angle < -157.5)) {\n                // Horizontal\n                n1 = magnitude[idx - 1];\n                n2 = magnitude[idx + 1];\n            } else if ((angle >= 22.5 && angle < 67.5) || (angle >= -157.5 && angle < -112.5)) {\n                // Diagonal /\n                n1 = magnitude[(y-1)*width + (x+1)];\n                n2 = magnitude[(y+1)*width + (x-1)];\n            } else if ((angle >= 67.5 && angle < 112.5) || (angle >= -112.5 && angle < -67.5)) {\n                // Vertical\n                n1 = magnitude[(y-1)*width + x];\n                n2 = magnitude[(y+1)*width + x];\n            } else {\n                // Diagonal \\\n                n1 = magnitude[(y-1)*width + (x-1)];\n                n2 = magnitude[(y+1)*width + (x+1)];\n            }\n            \n            // Suprimir si no es máximo local\n            if (mag >= n1 && mag >= n2) {\n                suppressed[idx] = mag;\n            } else {\n                suppressed[idx] = 0;\n            }\n        }\n    }\n    \n    // 5. Umbralización con histéresis (doble umbral + seguimiento de bordes)\n    const lowThreshold = 30;\n    const highThreshold = 90;\n    const edges = new Uint8ClampedArray(width * height);\n    \n    // Marcar píxeles fuertes\n    for (let i = 0; i < suppressed.length; i++) {\n        if (suppressed[i] >= highThreshold) {\n            edges[i] = 255; // Borde fuerte\n        } else if (suppressed[i] >= lowThreshold) {\n            edges[i] = 128; // Borde débil (candidato)\n        } else {\n            edges[i] = 0;\n        }\n    }\n    \n    // Seguimiento de bordes (conectar bordes débiles a fuertes)\n    for (let y = 1; y < height - 1; y++) {\n        for (let x = 1; x < width - 1; x++) {\n            const idx = y * width + x;\n            \n            if (edges[idx] === 128) { // Borde débil\n                // Verificar si está conectado a un borde fuerte\n                let connected = false;\n                for (let dy = -1; dy <= 1; dy++) {\n                    for (let dx = -1; dx <= 1; dx++) {\n                        if (edges[(y+dy)*width + (x+dx)] === 255) {\n                            connected = true;\n                            break;\n                        }\n                    }\n                    if (connected) break;\n                }\n                \n                edges[idx] = connected ? 255 : 0;\n            }\n        }\n    }\n    \n    // Convertir a ImageData\n    const output = new ImageData(width, height);\n    for (let i = 0; i < edges.length; i++) {\n        output.data[i * 4] = edges[i];\n        output.data[i * 4 + 1] = edges[i];\n        output.data[i * 4 + 2] = edges[i];\n        output.data[i * 4 + 3] = 255;\n    }\n    \n    return output;\n}\n\nasync function iniciarCannyPro() {\n    const log = document.getElementById('log_canny_pro');\n    \n    detenerTodo();\n    \n    try {\n        log.innerText = \"Solicitando cámara...\";\n        \n        const stream = await navigator.mediaDevices.getUserMedia({ \n            video: { width: 640, height: 480, facingMode: 'user' } \n        });\n        \n        window.cameraManager.currentStream = stream;\n        window.cameraManager.currentFilter = 'canny';\n        \n        const v = document.getElementById('v_canny_pro');\n        const canvas = document.getElementById('c_canny_pro');\n        const ctx = canvas.getContext('2d');\n        \n        v.srcObject = stream;\n        await v.play();\n        \n        await new Promise(resolve => setTimeout(resolve, 500));\n        \n        document.getElementById('btn_on_canny').style.display = 'none';\n        document.getElementById('btn_off_canny').style.display = 'inline-block';\n        log.innerText = \"Detección de bordes Canny activa\";\n        \n        window.cameraManager.activeLoop = true;\n        let frameCount = 0;\n        \n        function processFrame() {\n            if (!window.cameraManager.activeLoop || window.cameraManager.currentFilter !== 'canny') {\n                return;\n            }\n            \n            try {\n                ctx.drawImage(v, 0, 0, canvas.width, canvas.height);\n                const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);\n                \n                // Aplicar CANNY REAL\n                const edges = applyCannyEdgeDetection(imageData);\n                \n                ctx.putImageData(edges, 0, 0);\n                \n                frameCount++;\n                if (frameCount % 30 === 0) {\n                    log.innerText = \"Canny activo - Frames: \" + frameCount;\n                }\n                \n                requestAnimationFrame(processFrame);\n                \n            } catch (e) {\n                console.error('Error:', e);\n                log.innerText = \"Error: \" + e.message;\n            }\n        }\n        \n        processFrame();\n        \n    } catch (e) {\n        log.innerText = \"Error: \" + e.message;\n        console.error(e);\n    }\n}\n</script>",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<div style=\"text-align: center; background: #1a1a1a; padding: 20px; border-radius: 15px; color: white; font-family: sans-serif;\">\n    <h3>Método Canny Edge(Derivadas y Bordes)</h3>\n    <video id=\"v_canny_pro\" width=\"640\" height=\"480\" style=\"display:none\" playsinline></video>\n    <canvas id=\"c_canny_pro\" width=\"640\" height=\"480\" style=\"background: #000; border: 2px solid #27ae60; border-radius: 10px;\"></canvas>\n    <div style=\"margin-top: 15px;\">\n        <button id=\"btn_on_canny\" onclick=\"iniciarCannyPro()\" style=\"padding: 10px 20px; background: #27ae60; color: white; border: none; border-radius: 5px; cursor: pointer; font-weight: bold;\">ACTIVAR CANNY</button>\n        <button id=\"btn_off_canny\" onclick=\"detenerTodo()\" style=\"padding: 10px 20px; background: #e74c3c; color: white; border: none; border-radius: 5px; cursor: pointer; margin-left: 10px; display: none;\">APAGAR</button>\n    </div>\n    <p id=\"log_canny_pro\" style=\"color: #f1c40f; font-size: 13px; margin-top: 10px;\">Estado: Esperando cámara...</p>\n</div>\n\n<script>\n// SISTEMA GLOBAL DE GESTIÓN DE CÁMARA\nif (typeof window.cameraManager === 'undefined') {\n    window.cameraManager = {\n        currentStream: null,\n        currentFilter: null,\n        activeLoop: null\n    };\n}\n\nfunction detenerTodo() {\n    if (window.cameraManager.activeLoop) {\n        window.cameraManager.activeLoop = false;\n    }\n\n    if (window.cameraManager.currentStream) {\n        window.cameraManager.currentStream.getTracks().forEach(t => t.stop());\n        window.cameraManager.currentStream = null;\n    }\n\n    // LIMPIAR TODOS LOS CANVAS\n    ['c_canny_pro', 'c_out', 'c_sobel_pro'].forEach(canvasId => {\n        const canvas = document.getElementById(canvasId);\n        if (canvas) {\n            const ctx = canvas.getContext('2d');\n            ctx.clearRect(0, 0, canvas.width, canvas.height);\n            ctx.fillStyle = '#000';\n            ctx.fillRect(0, 0, canvas.width, canvas.height);\n        }\n    });\n\n    ['btn_off_canny', 'btn_off_sobel', 'b_stop'].forEach(id => {\n        const btn = document.getElementById(id);\n        if (btn) btn.style.display = 'none';\n    });\n\n    ['btn_on_canny', 'btn_on_sobel', 'b_start'].forEach(id => {\n        const btn = document.getElementById(id);\n        if (btn) btn.style.display = 'inline-block';\n    });\n\n    ['log_canny_pro', 'log_sobel_pro', 'debug_log'].forEach(id => {\n        const log = document.getElementById(id);\n        if (log) log.innerText = \"Cámara liberada.\";\n    });\n\n    window.cameraManager.currentFilter = null;\n}\n\n// Implementación REAL de Canny Edge Detection\nfunction applyCannyEdgeDetection(imageData) {\n    const width = imageData.width;\n    const height = imageData.height;\n    const data = imageData.data;\n\n    // 1. Convertir a escala de grises\n    const gray = new Uint8ClampedArray(width * height);\n    for (let i = 0; i < data.length; i += 4) {\n        const idx = i / 4;\n        gray[idx] = 0.299 * data[i] + 0.587 * data[i + 1] + 0.114 * data[i + 2];\n    }\n\n    // 2. Suavizado Gaussiano 5x5\n    const smoothed = new Uint8ClampedArray(width * height);\n    const gaussianKernel = [\n        2, 4, 5, 4, 2,\n        4, 9, 12, 9, 4,\n        5, 12, 15, 12, 5,\n        4, 9, 12, 9, 4,\n        2, 4, 5, 4, 2\n    ];\n    const kernelSum = 159;\n\n    for (let y = 2; y < height - 2; y++) {\n        for (let x = 2; x < width - 2; x++) {\n            let sum = 0;\n            for (let ky = -2; ky <= 2; ky++) {\n                for (let kx = -2; kx <= 2; kx++) {\n                    const idx = (y + ky) * width + (x + kx);\n                    sum += gray[idx] * gaussianKernel[(ky + 2) * 5 + (kx + 2)];\n                }\n            }\n            smoothed[y * width + x] = sum / kernelSum;\n        }\n    }\n\n    // 3. Calcular gradientes con Sobel\n    const gradX = new Float32Array(width * height);\n    const gradY = new Float32Array(width * height);\n    const magnitude = new Float32Array(width * height);\n    const direction = new Float32Array(width * height);\n\n    for (let y = 1; y < height - 1; y++) {\n        for (let x = 1; x < width - 1; x++) {\n            const idx = y * width + x;\n\n            // Sobel X\n            const gx = (\n                -smoothed[(y-1)*width + (x-1)] + smoothed[(y-1)*width + (x+1)] +\n                -2*smoothed[y*width + (x-1)] + 2*smoothed[y*width + (x+1)] +\n                -smoothed[(y+1)*width + (x-1)] + smoothed[(y+1)*width + (x+1)]\n            );\n\n            // Sobel Y\n            const gy = (\n                -smoothed[(y-1)*width + (x-1)] - 2*smoothed[(y-1)*width + x] - smoothed[(y-1)*width + (x+1)] +\n                smoothed[(y+1)*width + (x-1)] + 2*smoothed[(y+1)*width + x] + smoothed[(y+1)*width + (x+1)]\n            );\n\n            gradX[idx] = gx;\n            gradY[idx] = gy;\n            magnitude[idx] = Math.sqrt(gx * gx + gy * gy);\n            direction[idx] = Math.atan2(gy, gx);\n        }\n    }\n\n    // 4. Supresión no-máxima (esto hace que Canny sea diferente de Sobel)\n    const suppressed = new Float32Array(width * height);\n\n    for (let y = 1; y < height - 1; y++) {\n        for (let x = 1; x < width - 1; x++) {\n            const idx = y * width + x;\n            const angle = direction[idx] * 180 / Math.PI;\n            const mag = magnitude[idx];\n\n            let n1 = 0, n2 = 0;\n\n            // Determinar vecinos según la dirección del gradiente\n            if ((angle >= -22.5 && angle < 22.5) || (angle >= 157.5 || angle < -157.5)) {\n                // Horizontal\n                n1 = magnitude[idx - 1];\n                n2 = magnitude[idx + 1];\n            } else if ((angle >= 22.5 && angle < 67.5) || (angle >= -157.5 && angle < -112.5)) {\n                // Diagonal /\n                n1 = magnitude[(y-1)*width + (x+1)];\n                n2 = magnitude[(y+1)*width + (x-1)];\n            } else if ((angle >= 67.5 && angle < 112.5) || (angle >= -112.5 && angle < -67.5)) {\n                // Vertical\n                n1 = magnitude[(y-1)*width + x];\n                n2 = magnitude[(y+1)*width + x];\n            } else {\n                // Diagonal \\\n                n1 = magnitude[(y-1)*width + (x-1)];\n                n2 = magnitude[(y+1)*width + (x+1)];\n            }\n\n            // Suprimir si no es máximo local\n            if (mag >= n1 && mag >= n2) {\n                suppressed[idx] = mag;\n            } else {\n                suppressed[idx] = 0;\n            }\n        }\n    }\n\n    // 5. Umbralización con histéresis (doble umbral + seguimiento de bordes)\n    const lowThreshold = 30;\n    const highThreshold = 90;\n    const edges = new Uint8ClampedArray(width * height);\n\n    // Marcar píxeles fuertes\n    for (let i = 0; i < suppressed.length; i++) {\n        if (suppressed[i] >= highThreshold) {\n            edges[i] = 255; // Borde fuerte\n        } else if (suppressed[i] >= lowThreshold) {\n            edges[i] = 128; // Borde débil (candidato)\n        } else {\n            edges[i] = 0;\n        }\n    }\n\n    // Seguimiento de bordes (conectar bordes débiles a fuertes)\n    for (let y = 1; y < height - 1; y++) {\n        for (let x = 1; x < width - 1; x++) {\n            const idx = y * width + x;\n\n            if (edges[idx] === 128) { // Borde débil\n                // Verificar si está conectado a un borde fuerte\n                let connected = false;\n                for (let dy = -1; dy <= 1; dy++) {\n                    for (let dx = -1; dx <= 1; dx++) {\n                        if (edges[(y+dy)*width + (x+dx)] === 255) {\n                            connected = true;\n                            break;\n                        }\n                    }\n                    if (connected) break;\n                }\n\n                edges[idx] = connected ? 255 : 0;\n            }\n        }\n    }\n\n    // Convertir a ImageData\n    const output = new ImageData(width, height);\n    for (let i = 0; i < edges.length; i++) {\n        output.data[i * 4] = edges[i];\n        output.data[i * 4 + 1] = edges[i];\n        output.data[i * 4 + 2] = edges[i];\n        output.data[i * 4 + 3] = 255;\n    }\n\n    return output;\n}\n\nasync function iniciarCannyPro() {\n    const log = document.getElementById('log_canny_pro');\n\n    detenerTodo();\n\n    try {\n        log.innerText = \"Solicitando cámara...\";\n\n        const stream = await navigator.mediaDevices.getUserMedia({ \n            video: { width: 640, height: 480, facingMode: 'user' } \n        });\n\n        window.cameraManager.currentStream = stream;\n        window.cameraManager.currentFilter = 'canny';\n\n        const v = document.getElementById('v_canny_pro');\n        const canvas = document.getElementById('c_canny_pro');\n        const ctx = canvas.getContext('2d');\n\n        v.srcObject = stream;\n        await v.play();\n\n        await new Promise(resolve => setTimeout(resolve, 500));\n\n        document.getElementById('btn_on_canny').style.display = 'none';\n        document.getElementById('btn_off_canny').style.display = 'inline-block';\n        log.innerText = \"Detección de bordes Canny activa\";\n\n        window.cameraManager.activeLoop = true;\n        let frameCount = 0;\n\n        function processFrame() {\n            if (!window.cameraManager.activeLoop || window.cameraManager.currentFilter !== 'canny') {\n                return;\n            }\n\n            try {\n                ctx.drawImage(v, 0, 0, canvas.width, canvas.height);\n                const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);\n\n                // Aplicar CANNY REAL\n                const edges = applyCannyEdgeDetection(imageData);\n\n                ctx.putImageData(edges, 0, 0);\n\n                frameCount++;\n                if (frameCount % 30 === 0) {\n                    log.innerText = \"Canny activo - Frames: \" + frameCount;\n                }\n\n                requestAnimationFrame(processFrame);\n\n            } catch (e) {\n                console.error('Error:', e);\n                log.innerText = \"Error: \" + e.message;\n            }\n        }\n\n        processFrame();\n\n    } catch (e) {\n        log.innerText = \"Error: \" + e.message;\n        console.error(e);\n    }\n}\n</script>\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17
    },
    {
      "id": "243f3e5a-f4d8-4130-bf62-2c3c02c2d39d",
      "cell_type": "markdown",
      "source": "\n### 7. ¿El algoritmo Canny Edge Detection utiliza derivadas?\n[cite_start]**Sí.** En el contexto de una imagen, un borde se define matemáticamente como un cambio brusco en la intensidad de los píxeles[cite: 34]. [cite_start]El algoritmo de Canny busca los puntos donde la primera derivada de la función de intensidad de la imagen alcanza un máximo local (es decir, donde el gradiente es más pronunciado)[cite: 35].\n\n### 8. ¿En qué forma el algoritmo Canny Edge utiliza las diferencias finitas en su cálculo?\n[cite_start]Dado que una imagen digital es una matriz discreta de píxeles y no una función continua, no es posible calcular derivadas analíticas[cite: 38]. [cite_start]El algoritmo utiliza **Diferencias Finitas** para aproximar el gradiente[cite: 39]. [cite_start]Se aplican núcleos de convolución (como el operador Sobel) que realizan restas ponderadas entre píxeles vecinos (ej. $f(x+1)-f(x-1)$) para estimar la tasa de cambio (derivada) en las direcciones horizontal y vertical[cite: 39].\n\n### 9. Algoritmos para detectar bordes\n[cite_start]Existen diversos operadores basados en el cálculo del gradiente a través de diferencias finitas[cite: 41]:\n* [cite_start]Operador Sobel [cite: 42]\n* [cite_start]Operador Prewitt [cite: 43]\n* [cite_start]Operador Roberts [cite: 44]\n* [cite_start]Laplaciano de Gaussiana (LOG) [cite: 45]\n* [cite_start]Algoritmo de Canny [cite: 46]\n\n### 10. ¿Para qué sirve el algoritmo de Sobel?\n[cite_start]El operador Sobel sirve para calcular una aproximación del gradiente de intensidad de una imagen[cite: 48]. [cite_start]Se utiliza para detectar bordes resaltando las regiones de alta frecuencia espacial[cite: 49]. [cite_start]Es computacionalmente eficiente y efectivo para detectar la orientación y magnitud de los bordes simples[cite: 50].\n\n### 11 y 12. ¿Cómo utiliza Sobel las derivadas?\n[cite_start]Calcula la **Primera Derivada discreta** usando máscaras de $3\\times3$ para $G_{x}$ (horizontal) y $G_{y}$ (vertical)[cite: 52].\nLa magnitud total se obtiene con:\n[cite_start]$$G=\\sqrt{G_{x}^{2}+G_{y}^{2}}$$ [cite: 53]\n\n### 13. Relación de Sobel con diferencias finitas\nLa relación es directa. [cite_start]El núcleo aplica una **Diferencia Central** combinada con un suavizado[cite: 55]. [cite_start]Por ejemplo, el kernel `[-1, 0, +1]` es la definición numérica de la primera diferencia finita[cite: 56].\n\n### 14. ¿Se requiere escala de grises para Sobel?\n[cite_start]**Sí.** Los algoritmos de detección de bordes operan sobre cambios de intensidad (luminosidad), no sobre información cromática[cite: 59]. [cite_start]Convertir la imagen a escala de grises simplifica la entrada de 3 canales (RGB) a 1 canal, reduciendo la complejidad computacional y eliminando el ruido que podrían introducir las variaciones de tono[cite: 60].\n\n### 15. ¿MindAR es de fuente abierta?\n[cite_start]**Sí.** MindAR se distribuye bajo la licencia MIT[cite: 62]. [cite_start]Esto significa que es software libre y de código abierto, permitiendo su uso, modificación y distribución tanto para proyectos personales como comerciales sin restricciones significativas[cite: 63].\n\n### 16. ¿Qué es MediaPipe Face Mesh de Google?\n[cite_start]MediaPipe Face Mesh es una solución de aprendizaje automático (Machine Learning) desarrollada por Google que permite la estimación geométrica de rostros en tiempo real[cite: 65]. [cite_start]Es capaz de detectar **468 puntos de referencia (landmarks)** en 3D sobre el rostro humano, funcionando eficientemente incluso en dispositivos móviles sin hardware dedicado[cite: 66].\n\n### 17, 20, 21 y 22. Implementación Técnica: Face Mesh con Filtros y WebCam\n[cite_start]A continuación se presenta el código fuente que integra los requerimientos[cite: 76]:\n* [cite_start]**Punto 17:** Carga MediaPipe Face Mesh y dibuja los 468 puntos[cite: 78].\n* [cite_start]**Punto 20:** Solicitud de acceso a webcam[cite: 79].\n* [cite_start]**Punto 21:** Edición de máscara colocando un objeto[cite: 80].\n* [cite_start]**Punto 22:** Filtro en un punto específico (nariz)[cite: 81].\n\n# Análisis Matemático: Motor de Renderizado AR Vectorial\n\nPara este módulo final, nuestro equipo desarrolló un sistema de **Realidad Aumentada (AR)** que no depende de la superposición de imágenes estáticas (sprites). En su lugar, hemos programado un motor de renderizado geométrico que construye los objetos píxel a píxel en tiempo real, utilizando ecuaciones paramétricas y transformaciones lineales.\n\nA continuación, detallamos la lógica matemática que gobierna nuestro código:\n\n### 1. Transformación de Espacios (Mapeo Lineal)\nEl modelo de Inteligencia Artificial (MediaPipe Face Mesh) nos devuelve los puntos clave (landmarks) en un **Espacio Normalizado** $\\mathbb{R}^2_{[0,1]}$. Para poder dibujar en la pantalla, aplicamos una **Transformación Afín de Escala** para mapear estos valores al espacio discreto del Canvas ($640 \\times 480$).\n\nPara cualquier punto $P_n(x, y)$ detectado por la red neuronal, su posición en pantalla $P_s$ se calcula como:\n\n$$\nP_s = \\begin{bmatrix} W_{canvas} & 0 \\\\ 0 & H_{canvas} \\end{bmatrix} \\cdot P_n\n$$\n\n> **En nuestro código:** Esto se evidencia en instrucciones como `nose.x * can_el.width`.\n\n---\n\n### 2. Escalamiento Dinámico (Simulación de Profundidad Z)\nUno de los desafíos matemáticos fue lograr que los objetos (lentes, sombreros) cambiaran de tamaño coherentemente al acercarnos o alejarnos de la cámara. Sin un sensor de profundidad (LiDAR), utilizamos una **Métrica Relativa Euclideana**.\n\nSeleccionamos dos puntos anatómicamente rígidos: los pómulos izquierdo y derecho (Landmarks **#454** y **#234**). Calculamos la magnitud del vector que los une para derivar un factor de escala $k$:\n\n$$\nk = |x_{454} - x_{234}| \\cdot W_{canvas}\n$$\n\nEste escalar $k$ se propaga a todas las funciones de dibujo. Por ejemplo, el radio de la nariz se define como $r = 0.15 \\cdot k$. Esto garantiza que la geometría mantenga su **Proporcionalidad Homotética** sin importar la distancia del sujeto.\n\n---\n\n### 3. Construcción de Primitivas Geométricas\nEn lugar de cargar texturas, utilizamos **Geometría Analítica** para dibujar los filtros:\n\n#### A. Simulación Volumétrica (Nariz 3D)\nPara la \"Nariz Roja\", no dibujamos un círculo plano. Para simular una esfera 3D, implementamos una función de **Interpolación Radial de Color** (Gradiente).\nMatemáticamente, definimos una función de intensidad $I(r)$ que decae desde un punto focal desplazado $(x - \\Delta, y - \\Delta)$ hacia el borde. Esto simula la **Reflexión Especular** (brillo) de una fuente de luz, engañando al ojo para percibir volumen esférico.\n\n#### B. Polígonos y Vectores de Traslación (Corona y Sombrero)\nPara objetos complejos como la corona, definimos polígonos irregulares mediante una secuencia de vértices $V = \\{v_1, v_2, ..., v_n\\}$.\nEl reto es ubicar estos objetos *sobre* la cabeza, no *en* ella. Para ello, aplicamos un **Vector de Traslación Vertical** relativo al punto de anclaje (la frente, Landmark #10):\n\n$$\nP_{objeto} = P_{frente} + \\vec{v}_{offset}\n$$\n\nDonde $\\vec{v}_{offset} = (0, -0.6 \\cdot k)$. El signo negativo indica un desplazamiento hacia arriba en el sistema de coordenadas de la pantalla (donde Y crece hacia abajo).\n\n---\n\n### 4. Topología de Malla (Grafos)\nFinalmente, la \"Malla Verde\" que visualizamos mediante `drawConnectors` representa la **Matriz de Adyacencia** del grafo facial.\nEl rostro se modela como un grafo $G=(V,E)$, donde $V$ son los 468 vértices detectados y $E$ son las aristas que definen la triangulación de la superficie, permitiéndonos visualizar la estructura topológica que la IA \"entiende\" del rostro humano.\n\n",
      "metadata": {}
    },
    {
      "id": "c8ad0994-3d41-40ba-9e19-2a01b591d38b",
      "cell_type": "code",
      "source": "%%html\n<div id=\"ar_final_box\" style=\"text-align: center; background: #1a1a1a; padding: 20px; border-radius: 15px; color: white; font-family: sans-serif;\">\n    <h2 style=\"color: #3498db;\">PROYECTO FINAL: Filtros AR 3D</h2>\n    \n    <div style=\"margin-bottom: 10px; display: flex; justify-content: center; gap: 5px; flex-wrap: wrap;\">\n        <button onclick=\"setF(0)\">Quitar</button>\n        <button onclick=\"setF(1)\">Nariz Roja</button>\n        <button onclick=\"setF(2)\">Sombrero</button>\n        <button onclick=\"setF(3)\">Lentes</button>\n        <button onclick=\"setF(4)\">Corona</button>\n    </div>\n\n    <div style=\"margin-bottom: 15px;\">\n        <button id=\"m_btn\" onclick=\"togM()\" style=\"padding: 8px 15px; background: #8e44ad; color: white; border: none; border-radius: 5px; cursor: pointer;\">Ocultar Malla Verde</button>\n    </div>\n\n    <div style=\"position: relative; display: inline-block;\">\n        <video id=\"v_src\" style=\"display:none\" playsinline></video>\n        <canvas id=\"c_out\" width=\"640\" height=\"480\" style=\"background: #000; border: 2px solid #444; border-radius: 10px;\"></canvas>\n    </div>\n    \n    <div style=\"margin-top: 15px;\">\n        <button id=\"b_start\" onclick=\"runAr()\" style=\"padding: 15px 30px; background: #27ae60; color: white; border: none; border-radius: 10px; cursor: pointer; font-weight: bold;\">ACTIVAR FILTROS AR</button>\n        <button onclick=\"detenerTodo()\" style=\"padding: 15px 30px; background: #e74c3c; color: white; border: none; border-radius: 10px; cursor: pointer; margin-left: 10px; display: none;\" id=\"b_stop\">APAGAR</button>\n        <p id=\"debug_log\" style=\"color: #f1c40f; font-size: 13px; margin-top: 10px; background: #000; padding: 5px;\">Estado: Esperando clic...</p>\n    </div>\n</div>\n\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js\" crossorigin=\"anonymous\"></script>\n\n<script>\nvar vid_el = document.getElementById('v_src');\nvar can_el = document.getElementById('c_out');\nvar ctx_el = can_el.getContext('2d');\nvar log_el = document.getElementById('debug_log');\n\nvar show_m = true;\nvar filter_id = 0;\nvar arCamera = null;\n\nfunction setF(n) {\n    filter_id = n;\n    log_el.innerText = n > 0 ? \"Filtro \" + n + \" activado ✓\" : \"Filtro removido\";\n}\n\nfunction togM() {\n    show_m = !show_m;\n    document.getElementById('m_btn').innerText = show_m ? \"Ocultar Malla Verde\" : \"Mostrar Malla Verde\";\n}\n\n// FUNCIONES PARA DIBUJAR FILTROS\nfunction drawClownNose(ctx, x, y, size) {\n    ctx.save();\n    ctx.shadowColor = 'rgba(0,0,0,0.4)';\n    ctx.shadowBlur = 25;\n    ctx.shadowOffsetX = 15;\n    ctx.shadowOffsetY = 15;\n    \n    const gradient = ctx.createRadialGradient(x - size*0.2, y - size*0.2, 0, x, y, size*0.6);\n    gradient.addColorStop(0, '#ff6b6b');\n    gradient.addColorStop(0.5, '#ee4444');\n    gradient.addColorStop(1, '#cc2222');\n    \n    ctx.fillStyle = gradient;\n    ctx.beginPath();\n    ctx.arc(x, y, size*0.25, 0, Math.PI * 2);\n    ctx.fill();\n    \n    ctx.shadowColor = 'transparent';\n    const highlightGrad = ctx.createRadialGradient(x - size*0.15, y - size*0.15, 0, x - size*0.15, y - size*0.15, size*0.25);\n    highlightGrad.addColorStop(0, 'rgba(255,255,255,0.9)');\n    highlightGrad.addColorStop(1, 'rgba(255,255,255,0)');\n    ctx.fillStyle = highlightGrad;\n    ctx.beginPath();\n    ctx.arc(x - size*0.15, y - size*0.15, size*0.25, 0, Math.PI * 2);\n    ctx.fill();\n    \n    ctx.fillStyle = 'rgba(255,255,255,0.6)';\n    ctx.beginPath();\n    ctx.arc(x + size*0.1, y + size*0.15, size*0.08, 0, Math.PI * 2);\n    ctx.fill();\n    ctx.restore();\n}\n\nfunction drawHat(ctx, x, y, size) {\n    ctx.save();\n    ctx.fillStyle = '#8B4513';\n    ctx.strokeStyle = '#654321';\n    ctx.lineWidth = 3;\n    ctx.shadowColor = 'rgba(0,0,0,0.5)';\n    ctx.shadowBlur = 15;\n    \n    ctx.beginPath();\n    ctx.ellipse(x, y, size*0.8, size*0.25, 0, 0, Math.PI * 2);\n    ctx.fill();\n    ctx.stroke();\n    \n    ctx.fillStyle = '#A0522D';\n    ctx.beginPath();\n    ctx.moveTo(x - size*0.4, y);\n    ctx.lineTo(x - size*0.35, y - size*0.8);\n    ctx.lineTo(x + size*0.35, y - size*0.8);\n    ctx.lineTo(x + size*0.4, y);\n    ctx.closePath();\n    ctx.fill();\n    ctx.stroke();\n    \n    ctx.fillStyle = '#FFD700';\n    ctx.fillRect(x - size*0.35, y - size*0.3, size*0.7, size*0.15);\n    ctx.restore();\n}\n\nfunction drawGlasses(ctx, x, y, size) {\n    ctx.save();\n    ctx.strokeStyle = '#000000';\n    ctx.lineWidth = 4;\n    ctx.shadowColor = 'rgba(0,0,0,0.5)';\n    ctx.shadowBlur = 10;\n    \n    ctx.fillStyle = 'rgba(0,0,0,0.3)';\n    ctx.beginPath();\n    ctx.arc(x - size*0.35, y, size*0.25, 0, Math.PI * 2);\n    ctx.fill();\n    ctx.stroke();\n    \n    ctx.beginPath();\n    ctx.arc(x + size*0.35, y, size*0.25, 0, Math.PI * 2);\n    ctx.fill();\n    ctx.stroke();\n    \n    ctx.beginPath();\n    ctx.moveTo(x - size*0.1, y);\n    ctx.lineTo(x + size*0.1, y);\n    ctx.stroke();\n    \n    ctx.fillStyle = 'rgba(255,255,255,0.6)';\n    ctx.beginPath();\n    ctx.arc(x - size*0.42, y - size*0.08, size*0.08, 0, Math.PI * 2);\n    ctx.fill();\n    ctx.beginPath();\n    ctx.arc(x + size*0.28, y - size*0.08, size*0.08, 0, Math.PI * 2);\n    ctx.fill();\n    ctx.restore();\n}\n\nfunction drawCrown(ctx, x, y, size) {\n    ctx.save();\n    ctx.fillStyle = '#FFD700';\n    ctx.strokeStyle = '#FFA500';\n    ctx.lineWidth = 3;\n    ctx.shadowColor = 'rgba(0,0,0,0.5)';\n    ctx.shadowBlur = 15;\n    \n    ctx.beginPath();\n    ctx.moveTo(x - size*0.5, y);\n    ctx.lineTo(x - size*0.4, y - size*0.4);\n    ctx.lineTo(x - size*0.25, y - size*0.2);\n    ctx.lineTo(x, y - size*0.5);\n    ctx.lineTo(x + size*0.25, y - size*0.2);\n    ctx.lineTo(x + size*0.4, y - size*0.4);\n    ctx.lineTo(x + size*0.5, y);\n    ctx.closePath();\n    ctx.fill();\n    ctx.stroke();\n    \n    const jewels = [\n        {x: x - size*0.4, y: y - size*0.4, c: '#ff0000'},\n        {x: x - size*0.25, y: y - size*0.2, c: '#00ff00'},\n        {x: x, y: y - size*0.5, c: '#ff0000'},\n        {x: x + size*0.25, y: y - size*0.2, c: '#0000ff'},\n        {x: x + size*0.4, y: y - size*0.4, c: '#ff00ff'}\n    ];\n    \n    jewels.forEach(j => {\n        ctx.fillStyle = j.c;\n        ctx.beginPath();\n        ctx.arc(j.x, j.y, size*0.05, 0, Math.PI * 2);\n        ctx.fill();\n    });\n    ctx.restore();\n}\n\nasync function runAr() {\n    log_el.innerText = \"Paso 1: Solicitando cámara...\";\n    \n    // Detener otros filtros\n    detenerTodo();\n    \n    try {\n        const stream = await navigator.mediaDevices.getUserMedia({ video: { width: 640, height: 480 } });\n        window.cameraManager.currentStream = stream;\n        window.cameraManager.currentFilter = 'ar';\n        \n        vid_el.srcObject = stream;\n        await vid_el.play();\n        \n        document.getElementById('b_start').style.display = 'none';\n        document.getElementById('b_stop').style.display = 'inline-block';\n        \n        log_el.innerText = \"Paso 2: Cámara activa. Cargando IA...\";\n        setTimeout(startIA, 500);\n    } catch (e) {\n        log_el.innerText = \"ERROR: No se pudo abrir la cámara. ¿Diste permiso?\";\n        console.error(e);\n    }\n}\n\nfunction startIA() {\n    try {\n        const mesh = new FaceMesh({\n            locateFile: (f) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${f}`\n        });\n        \n        mesh.setOptions({ \n            maxNumFaces: 1, \n            refineLandmarks: true, \n            minDetectionConfidence: 0.5,\n            minTrackingConfidence: 0.5\n        });\n        \n        mesh.onResults((res) => {\n            if (window.cameraManager.currentFilter !== 'ar') return;\n            \n            ctx_el.save();\n            ctx_el.clearRect(0, 0, can_el.width, can_el.height);\n            ctx_el.drawImage(res.image, 0, 0, can_el.width, can_el.height);\n            \n            if (res.multiFaceLandmarks && res.multiFaceLandmarks.length > 0) {\n                const face = res.multiFaceLandmarks[0];\n                \n                if (show_m) {\n                    drawConnectors(ctx_el, face, FACEMESH_TESSELATION, {color: '#00FF0050', lineWidth: 1});\n                }\n\n                const faceW = Math.abs(face[454].x - face[234].x) * can_el.width;\n                \n                if (filter_id === 1) {\n                    const nose = face[1];\n                    drawClownNose(ctx_el, nose.x * can_el.width, nose.y * can_el.height, faceW * 0.15);\n                } \n                else if (filter_id === 2) {\n                    const top = face[10];\n                    drawHat(ctx_el, top.x * can_el.width, (top.y * can_el.height) - faceW*0.6, faceW * 0.8);\n                }\n                else if (filter_id === 3) {\n                    const eyes = face[168];\n                    drawGlasses(ctx_el, eyes.x * can_el.width, eyes.y * can_el.height, faceW);\n                }\n                else if (filter_id === 4) {\n                    const top = face[10];\n                    drawCrown(ctx_el, top.x * can_el.width, (top.y * can_el.height) - faceW*0.5, faceW * 0.6);\n                }\n            }\n            ctx_el.restore();\n        });\n\n        arCamera = new Camera(vid_el, {\n            onFrame: async () => {\n                if (window.cameraManager.currentFilter === 'ar') {\n                    await mesh.send({image: vid_el});\n                }\n            },\n            width: 640, \n            height: 480\n        });\n        \n        arCamera.start();\n        log_el.innerText = \"¡Todo listo! Selecciona un filtro\";\n        \n    } catch (err) {\n        log_el.innerText = \"ERROR de IA: \" + err.message;\n        console.error(err);\n    }\n}\n</script>",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<div id=\"ar_final_box\" style=\"text-align: center; background: #1a1a1a; padding: 20px; border-radius: 15px; color: white; font-family: sans-serif;\">\n    <h2 style=\"color: #3498db;\">PROYECTO FINAL: Filtros AR 3D</h2>\n\n    <div style=\"margin-bottom: 10px; display: flex; justify-content: center; gap: 5px; flex-wrap: wrap;\">\n        <button onclick=\"setF(0)\">Quitar</button>\n        <button onclick=\"setF(1)\">Nariz Roja</button>\n        <button onclick=\"setF(2)\">Sombrero</button>\n        <button onclick=\"setF(3)\">Lentes</button>\n        <button onclick=\"setF(4)\">Corona</button>\n    </div>\n\n    <div style=\"margin-bottom: 15px;\">\n        <button id=\"m_btn\" onclick=\"togM()\" style=\"padding: 8px 15px; background: #8e44ad; color: white; border: none; border-radius: 5px; cursor: pointer;\">Ocultar Malla Verde</button>\n    </div>\n\n    <div style=\"position: relative; display: inline-block;\">\n        <video id=\"v_src\" style=\"display:none\" playsinline></video>\n        <canvas id=\"c_out\" width=\"640\" height=\"480\" style=\"background: #000; border: 2px solid #444; border-radius: 10px;\"></canvas>\n    </div>\n\n    <div style=\"margin-top: 15px;\">\n        <button id=\"b_start\" onclick=\"runAr()\" style=\"padding: 15px 30px; background: #27ae60; color: white; border: none; border-radius: 10px; cursor: pointer; font-weight: bold;\">ACTIVAR FILTROS AR</button>\n        <button onclick=\"detenerTodo()\" style=\"padding: 15px 30px; background: #e74c3c; color: white; border: none; border-radius: 10px; cursor: pointer; margin-left: 10px; display: none;\" id=\"b_stop\">APAGAR</button>\n        <p id=\"debug_log\" style=\"color: #f1c40f; font-size: 13px; margin-top: 10px; background: #000; padding: 5px;\">Estado: Esperando clic...</p>\n    </div>\n</div>\n\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js\" crossorigin=\"anonymous\"></script>\n\n<script>\nvar vid_el = document.getElementById('v_src');\nvar can_el = document.getElementById('c_out');\nvar ctx_el = can_el.getContext('2d');\nvar log_el = document.getElementById('debug_log');\n\nvar show_m = true;\nvar filter_id = 0;\nvar arCamera = null;\n\nfunction setF(n) {\n    filter_id = n;\n    log_el.innerText = n > 0 ? \"Filtro \" + n + \" activado ✓\" : \"Filtro removido\";\n}\n\nfunction togM() {\n    show_m = !show_m;\n    document.getElementById('m_btn').innerText = show_m ? \"Ocultar Malla Verde\" : \"Mostrar Malla Verde\";\n}\n\n// FUNCIONES PARA DIBUJAR FILTROS\nfunction drawClownNose(ctx, x, y, size) {\n    ctx.save();\n    ctx.shadowColor = 'rgba(0,0,0,0.4)';\n    ctx.shadowBlur = 25;\n    ctx.shadowOffsetX = 15;\n    ctx.shadowOffsetY = 15;\n\n    const gradient = ctx.createRadialGradient(x - size*0.2, y - size*0.2, 0, x, y, size*0.6);\n    gradient.addColorStop(0, '#ff6b6b');\n    gradient.addColorStop(0.5, '#ee4444');\n    gradient.addColorStop(1, '#cc2222');\n\n    ctx.fillStyle = gradient;\n    ctx.beginPath();\n    ctx.arc(x, y, size*0.25, 0, Math.PI * 2);\n    ctx.fill();\n\n    ctx.shadowColor = 'transparent';\n    const highlightGrad = ctx.createRadialGradient(x - size*0.15, y - size*0.15, 0, x - size*0.15, y - size*0.15, size*0.25);\n    highlightGrad.addColorStop(0, 'rgba(255,255,255,0.9)');\n    highlightGrad.addColorStop(1, 'rgba(255,255,255,0)');\n    ctx.fillStyle = highlightGrad;\n    ctx.beginPath();\n    ctx.arc(x - size*0.15, y - size*0.15, size*0.25, 0, Math.PI * 2);\n    ctx.fill();\n\n    ctx.fillStyle = 'rgba(255,255,255,0.6)';\n    ctx.beginPath();\n    ctx.arc(x + size*0.1, y + size*0.15, size*0.08, 0, Math.PI * 2);\n    ctx.fill();\n    ctx.restore();\n}\n\nfunction drawHat(ctx, x, y, size) {\n    ctx.save();\n    ctx.fillStyle = '#8B4513';\n    ctx.strokeStyle = '#654321';\n    ctx.lineWidth = 3;\n    ctx.shadowColor = 'rgba(0,0,0,0.5)';\n    ctx.shadowBlur = 15;\n\n    ctx.beginPath();\n    ctx.ellipse(x, y, size*0.8, size*0.25, 0, 0, Math.PI * 2);\n    ctx.fill();\n    ctx.stroke();\n\n    ctx.fillStyle = '#A0522D';\n    ctx.beginPath();\n    ctx.moveTo(x - size*0.4, y);\n    ctx.lineTo(x - size*0.35, y - size*0.8);\n    ctx.lineTo(x + size*0.35, y - size*0.8);\n    ctx.lineTo(x + size*0.4, y);\n    ctx.closePath();\n    ctx.fill();\n    ctx.stroke();\n\n    ctx.fillStyle = '#FFD700';\n    ctx.fillRect(x - size*0.35, y - size*0.3, size*0.7, size*0.15);\n    ctx.restore();\n}\n\nfunction drawGlasses(ctx, x, y, size) {\n    ctx.save();\n    ctx.strokeStyle = '#000000';\n    ctx.lineWidth = 4;\n    ctx.shadowColor = 'rgba(0,0,0,0.5)';\n    ctx.shadowBlur = 10;\n\n    ctx.fillStyle = 'rgba(0,0,0,0.3)';\n    ctx.beginPath();\n    ctx.arc(x - size*0.35, y, size*0.25, 0, Math.PI * 2);\n    ctx.fill();\n    ctx.stroke();\n\n    ctx.beginPath();\n    ctx.arc(x + size*0.35, y, size*0.25, 0, Math.PI * 2);\n    ctx.fill();\n    ctx.stroke();\n\n    ctx.beginPath();\n    ctx.moveTo(x - size*0.1, y);\n    ctx.lineTo(x + size*0.1, y);\n    ctx.stroke();\n\n    ctx.fillStyle = 'rgba(255,255,255,0.6)';\n    ctx.beginPath();\n    ctx.arc(x - size*0.42, y - size*0.08, size*0.08, 0, Math.PI * 2);\n    ctx.fill();\n    ctx.beginPath();\n    ctx.arc(x + size*0.28, y - size*0.08, size*0.08, 0, Math.PI * 2);\n    ctx.fill();\n    ctx.restore();\n}\n\nfunction drawCrown(ctx, x, y, size) {\n    ctx.save();\n    ctx.fillStyle = '#FFD700';\n    ctx.strokeStyle = '#FFA500';\n    ctx.lineWidth = 3;\n    ctx.shadowColor = 'rgba(0,0,0,0.5)';\n    ctx.shadowBlur = 15;\n\n    ctx.beginPath();\n    ctx.moveTo(x - size*0.5, y);\n    ctx.lineTo(x - size*0.4, y - size*0.4);\n    ctx.lineTo(x - size*0.25, y - size*0.2);\n    ctx.lineTo(x, y - size*0.5);\n    ctx.lineTo(x + size*0.25, y - size*0.2);\n    ctx.lineTo(x + size*0.4, y - size*0.4);\n    ctx.lineTo(x + size*0.5, y);\n    ctx.closePath();\n    ctx.fill();\n    ctx.stroke();\n\n    const jewels = [\n        {x: x - size*0.4, y: y - size*0.4, c: '#ff0000'},\n        {x: x - size*0.25, y: y - size*0.2, c: '#00ff00'},\n        {x: x, y: y - size*0.5, c: '#ff0000'},\n        {x: x + size*0.25, y: y - size*0.2, c: '#0000ff'},\n        {x: x + size*0.4, y: y - size*0.4, c: '#ff00ff'}\n    ];\n\n    jewels.forEach(j => {\n        ctx.fillStyle = j.c;\n        ctx.beginPath();\n        ctx.arc(j.x, j.y, size*0.05, 0, Math.PI * 2);\n        ctx.fill();\n    });\n    ctx.restore();\n}\n\nasync function runAr() {\n    log_el.innerText = \"Paso 1: Solicitando cámara...\";\n\n    // Detener otros filtros\n    detenerTodo();\n\n    try {\n        const stream = await navigator.mediaDevices.getUserMedia({ video: { width: 640, height: 480 } });\n        window.cameraManager.currentStream = stream;\n        window.cameraManager.currentFilter = 'ar';\n\n        vid_el.srcObject = stream;\n        await vid_el.play();\n\n        document.getElementById('b_start').style.display = 'none';\n        document.getElementById('b_stop').style.display = 'inline-block';\n\n        log_el.innerText = \"Paso 2: Cámara activa. Cargando IA...\";\n        setTimeout(startIA, 500);\n    } catch (e) {\n        log_el.innerText = \"ERROR: No se pudo abrir la cámara. ¿Diste permiso?\";\n        console.error(e);\n    }\n}\n\nfunction startIA() {\n    try {\n        const mesh = new FaceMesh({\n            locateFile: (f) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${f}`\n        });\n\n        mesh.setOptions({ \n            maxNumFaces: 1, \n            refineLandmarks: true, \n            minDetectionConfidence: 0.5,\n            minTrackingConfidence: 0.5\n        });\n\n        mesh.onResults((res) => {\n            if (window.cameraManager.currentFilter !== 'ar') return;\n\n            ctx_el.save();\n            ctx_el.clearRect(0, 0, can_el.width, can_el.height);\n            ctx_el.drawImage(res.image, 0, 0, can_el.width, can_el.height);\n\n            if (res.multiFaceLandmarks && res.multiFaceLandmarks.length > 0) {\n                const face = res.multiFaceLandmarks[0];\n\n                if (show_m) {\n                    drawConnectors(ctx_el, face, FACEMESH_TESSELATION, {color: '#00FF0050', lineWidth: 1});\n                }\n\n                const faceW = Math.abs(face[454].x - face[234].x) * can_el.width;\n\n                if (filter_id === 1) {\n                    const nose = face[1];\n                    drawClownNose(ctx_el, nose.x * can_el.width, nose.y * can_el.height, faceW * 0.15);\n                } \n                else if (filter_id === 2) {\n                    const top = face[10];\n                    drawHat(ctx_el, top.x * can_el.width, (top.y * can_el.height) - faceW*0.6, faceW * 0.8);\n                }\n                else if (filter_id === 3) {\n                    const eyes = face[168];\n                    drawGlasses(ctx_el, eyes.x * can_el.width, eyes.y * can_el.height, faceW);\n                }\n                else if (filter_id === 4) {\n                    const top = face[10];\n                    drawCrown(ctx_el, top.x * can_el.width, (top.y * can_el.height) - faceW*0.5, faceW * 0.6);\n                }\n            }\n            ctx_el.restore();\n        });\n\n        arCamera = new Camera(vid_el, {\n            onFrame: async () => {\n                if (window.cameraManager.currentFilter === 'ar') {\n                    await mesh.send({image: vid_el});\n                }\n            },\n            width: 640, \n            height: 480\n        });\n\n        arCamera.start();\n        log_el.innerText = \"¡Todo listo! Selecciona un filtro\";\n\n    } catch (err) {\n        log_el.innerText = \"ERROR de IA: \" + err.message;\n        console.error(err);\n    }\n}\n</script>\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    },
    {
      "id": "a38d9007-9435-4b23-934d-32190095a305",
      "cell_type": "markdown",
      "source": "---\n### 18. ¿MediaPipe utiliza Sobel?\n[cite_start]**No directamente.** MediaPipe utiliza Redes Neuronales Convolucionales (CNN)[cite: 158]. [cite_start]A diferencia del algoritmo Sobel, que utiliza fórmulas matemáticas fijas (kernels predefinidos) para buscar bordes, MediaPipe utiliza modelos entrenados con millones de imágenes para aprender a identificar patrones complejos como ojos, labios y contornos faciales, independientemente de los bordes simples[cite: 158].\n\n# Análisis Matemático: Integración Unificada con MediaPipe Holistic\n\nPara la etapa final del proyecto, nuestro equipo implementó la solución **MediaPipe Holistic**. El desafío matemático aquí no es solo detectar puntos, sino resolver el problema de la **Inferencia Jerárquica** para procesar 543 puntos de referencia en tiempo real (30 FPS) sin colapsar el procesador.\n\nA continuación, describimos la arquitectura lógica que hemos desplegado:\n\n### 1. Arquitectura de Tubería (Pipeline Jerárquico)\nEn lugar de ejecutar tres redes neuronales independientes (lo cual sería computacionalmente costoso), utilizamos un enfoque de **Regiones de Interés (ROI)** derivado de la geometría proyectiva.\n\n1.  **Inferencia de Pose (Raíz):** Primero, el algoritmo detecta los 33 puntos del cuerpo.\n2.  **Cálculo de ROI:** Basándose en la ubicación matemática de las muñecas y el cuello, el sistema calcula recortes (crops) rotados y escalados de la imagen original.\n3.  **Inferencia Específica:** Estos recortes se alimentan a las sub-redes de **Manos** y **Face Mesh**.\n    * *Ventaja Numérica:* Si las manos no son visibles en la etapa de Pose, el sistema ahorra recursos al no ejecutar la red de manos (Gating).\n\n### 2. Espacios Vectoriales y Coordenadas 3D\nEl modelo nos devuelve vectores en un espacio métrico tridimensional $(x, y, z)$ para cada uno de los **543 landmarks**:\n* **Pose:** 33 puntos.\n* **Manos:** $21 \\times 2 = 42$ puntos.\n* **Rostro:** 468 puntos.\n\nPara cada punto $P_i$, el modelo predice:\n$$\nP_i = [x, y, z, v]\n$$\nDonde:\n* $x, y$: Coordenadas normalizadas $[0, 1]$ mapeadas al ancho y alto del canvas.\n* $z$: Profundidad relativa. En el modelo de pose, el origen $(z=0)$ es el punto medio de las caderas. Valores negativos indican que el punto está más cerca de la cámara.\n* $v$ (Visibilidad): Una probabilidad logística $[0, 1]$ que indica la certeza de que el punto es visible en la imagen (y no oculto por oclusión).\n\n### 3. Topología de Malla Densa (Face Mesh)\nPara el rostro, hemos activado `refineFaceLandmarks: true`. Esto utiliza una **Malla de Teselación** basada en la triangulación de Delaunay pre-calculada.\nMatemáticamente, esto nos permite mapear la superficie curva del rostro en un plano 2D sin perder la coherencia topológica, permitiendo detectar micro-movimientos en labios y ojos con vectores de conexión específicos (`FACEMESH_LIPS`, `FACEMESH_RIGHT_EYE`, etc.).\n\n### 4. Renderizado de Grafos Conectados\nEn la función de dibujo, tratamos los resultados como **Sub-Grafos Independientes**:\n* **Árbol de Manos:** Un grafo acíclico donde la \"muñeca\" es el nodo raíz, ramificándose en 5 cadenas cinemáticas (dedos).\n* **Malla Facial:** Un grafo cíclico denso diseñado para mantener la estructura estructural del óvalo facial.\n\nHemos implementado lógica condicional (`if (visibleParts.body)`, etc.) para optimizar el ciclo de renderizado, dibujando solo los tensores solicitados por el usuario y aplicando matrices de transformación de color distintas para diferenciar visualmente cada topología (Cian para cuerpo, Dorado para manos, Verde para rostro).\n\n\n",
      "metadata": {}
    },
    {
      "id": "397a8e55-0620-440a-824b-d7ae096dba69",
      "cell_type": "code",
      "source": "%%html\n<div style=\"text-align: center; background: #1a1a1a; padding: 20px; border-radius: 15px; color: white; font-family: sans-serif;\">\n    <h3>MediaPipe</h3>\n    <video id=\"v_pose_pro\" width=\"640\" height=\"480\" style=\"display:none\" playsinline></video>\n    <canvas id=\"c_pose_pro\" width=\"640\" height=\"480\" style=\"background: #000; border: 2px solid #9b59b6; border-radius: 10px;\"></canvas>\n    \n    <div style=\"margin-top: 15px;\">\n        <button id=\"btn_on_pose\" onclick=\"iniciarPosePro()\" style=\"padding: 10px 20px; background: #9b59b6; color: white; border: none; border-radius: 5px; cursor: pointer; font-weight: bold;\">ACTIVAR DETECCIÓN</button>\n        <button id=\"btn_off_pose\" onclick=\"detenerTodo()\" style=\"padding: 10px 20px; background: #e74c3c; color: white; border: none; border-radius: 5px; cursor: pointer; margin-left: 10px; display: none;\">APAGAR</button>\n    </div>\n    \n    <div style=\"margin-top: 10px; display: flex; justify-content: center; gap: 5px; flex-wrap: wrap;\">\n        <button onclick=\"togglePart('body')\" style=\"padding: 8px 15px; background: #3498db; color: white; border: none; border-radius: 5px; cursor: pointer;\">Cuerpo</button>\n        <button onclick=\"togglePart('hands')\" style=\"padding: 8px 15px; background: #f39c12; color: white; border: none; border-radius: 5px; cursor: pointer;\">Manos (21 puntos)</button>\n        <button onclick=\"togglePart('face')\" style=\"padding: 8px 15px; background: #e74c3c; color: white; border: none; border-radius: 5px; cursor: pointer;\">Rostro (468 puntos)</button>\n    </div>\n    \n    <p id=\"log_pose_pro\" style=\"color: #f1c40f; font-size: 13px; margin-top: 10px;\">Estado: Esperando cámara...</p>\n    <p style=\"color: #95a5a6; font-size: 11px; margin-top: 5px;\">Tip: Acerca tu mano o rostro para ver detalle máximo</p>\n</div>\n\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/holistic/holistic.js\" crossorigin=\"anonymous\"></script>\n\n<script>\n// Variables de configuración\nlet visibleParts = {\n    body: true,\n    hands: true,\n    face: true\n};\n\nlet holisticCamera = null;\n\nfunction togglePart(part) {\n    visibleParts[part] = !visibleParts[part];\n}\n\nasync function iniciarPosePro() {\n    const log = document.getElementById('log_pose_pro');\n    \n    detenerTodo();\n    \n    ['c_canny_pro', 'c_out', 'c_sobel_pro', 'c_pose_pro'].forEach(canvasId => {\n        const canvas = document.getElementById(canvasId);\n        if (canvas) {\n            const ctx = canvas.getContext('2d');\n            ctx.clearRect(0, 0, canvas.width, canvas.height);\n            ctx.fillStyle = '#000';\n            ctx.fillRect(0, 0, canvas.width, canvas.height);\n        }\n    });\n    \n    try {\n        log.innerText = \"Solicitando cámara...\";\n        \n        const stream = await navigator.mediaDevices.getUserMedia({ \n            video: { width: 640, height: 480, facingMode: 'user' } \n        });\n        \n        window.cameraManager.currentStream = stream;\n        window.cameraManager.currentFilter = 'pose';\n        \n        const v = document.getElementById('v_pose_pro');\n        const canvas = document.getElementById('c_pose_pro');\n        const ctx = canvas.getContext('2d');\n        \n        v.srcObject = stream;\n        await v.play();\n        \n        document.getElementById('btn_on_pose').style.display = 'none';\n        document.getElementById('btn_off_pose').style.display = 'inline-block';\n        \n        log.innerText = \"Cargando MediaPipe Holistic (cuerpo completo + manos + rostro)...\";\n        \n        // Usar HOLISTIC para obtener TODO con alta precisión\n        const holistic = new Holistic({\n            locateFile: (file) => {\n                return `https://cdn.jsdelivr.net/npm/@mediapipe/holistic/${file}`;\n            }\n        });\n        \n        holistic.setOptions({\n            modelComplexity: 2,\n            smoothLandmarks: true,\n            enableSegmentation: false,\n            smoothSegmentation: false,\n            refineFaceLandmarks: true, // ¡CLAVE! Rostro detallado\n            minDetectionConfidence: 0.5,\n            minTrackingConfidence: 0.5\n        });\n        \n        let frameCount = 0;\n        \n        holistic.onResults((results) => {\n            if (window.cameraManager.currentFilter !== 'pose') return;\n            \n            ctx.save();\n            ctx.clearRect(0, 0, canvas.width, canvas.height);\n            ctx.drawImage(results.image, 0, 0, canvas.width, canvas.height);\n            \n            let detections = [];\n            \n            // ========== CUERPO (33 puntos) ==========\n            if (visibleParts.body && results.poseLandmarks) {\n                // Esqueleto del cuerpo\n                drawConnectors(ctx, results.poseLandmarks, POSE_CONNECTIONS, {\n                    color: '#00FFFF',\n                    lineWidth: 4\n                });\n                \n                // Puntos del cuerpo\n                drawLandmarks(ctx, results.poseLandmarks, {\n                    color: '#00FFFF',\n                    fillColor: '#00FFFF',\n                    lineWidth: 2,\n                    radius: 5\n                });\n                \n                detections.push('Cuerpo (33 puntos)');\n            }\n            \n            // ========== MANOS IZQUIERDA (21 puntos cada una) ==========\n            if (visibleParts.hands) {\n                // Mano izquierda\n                if (results.leftHandLandmarks) {\n                    drawConnectors(ctx, results.leftHandLandmarks, HAND_CONNECTIONS, {\n                        color: '#FFD700',\n                        lineWidth: 3\n                    });\n                    drawLandmarks(ctx, results.leftHandLandmarks, {\n                        color: '#FFD700',\n                        fillColor: '#FFFF00',\n                        lineWidth: 2,\n                        radius: 4\n                    });\n                    detections.push('Mano Izq (21 puntos)');\n                }\n                \n                // Mano derecha\n                if (results.rightHandLandmarks) {\n                    drawConnectors(ctx, results.rightHandLandmarks, HAND_CONNECTIONS, {\n                        color: '#FF8C00',\n                        lineWidth: 3\n                    });\n                    drawLandmarks(ctx, results.rightHandLandmarks, {\n                        color: '#FF8C00',\n                        fillColor: '#FFA500',\n                        lineWidth: 2,\n                        radius: 4\n                    });\n                    detections.push('Mano Der (21 puntos)');\n                }\n            }\n            \n            // ========== ROSTRO (468 puntos - ¡SÍ, 468!) ==========\n            if (visibleParts.face && results.faceLandmarks) {\n                // Malla facial completa\n                drawConnectors(ctx, results.faceLandmarks, FACEMESH_TESSELATION, {\n                    color: '#00FF00',\n                    lineWidth: 0.5\n                });\n                \n                // Contornos importantes del rostro\n                drawConnectors(ctx, results.faceLandmarks, FACEMESH_RIGHT_EYE, {\n                    color: '#00FF00',\n                    lineWidth: 2\n                });\n                drawConnectors(ctx, results.faceLandmarks, FACEMESH_LEFT_EYE, {\n                    color: '#00FF00',\n                    lineWidth: 2\n                });\n                drawConnectors(ctx, results.faceLandmarks, FACEMESH_LIPS, {\n                    color: '#FF0000',\n                    lineWidth: 2\n                });\n                drawConnectors(ctx, results.faceLandmarks, FACEMESH_FACE_OVAL, {\n                    color: '#E0E0E0',\n                    lineWidth: 2\n                });\n                \n                // Puntos clave del rostro\n                drawLandmarks(ctx, results.faceLandmarks, {\n                    color: '#FF1493',\n                    fillColor: '#FF69B4',\n                    lineWidth: 1,\n                    radius: 1\n                });\n                \n                detections.push('Rostro (468 puntos)');\n            }\n            \n            frameCount++;\n            if (frameCount % 30 === 0) {\n                const detectedText = detections.length > 0 ? detections.join(' + ') : 'Esperando...';\n                log.innerText = `✅ ${detectedText} | Frames: ${frameCount}`;\n            }\n            \n            ctx.restore();\n        });\n        \n        // Iniciar cámara\n        holisticCamera = new Camera(v, {\n            onFrame: async () => {\n                if (window.cameraManager.currentFilter === 'pose') {\n                    await holistic.send({image: v});\n                }\n            },\n            width: 640,\n            height: 480\n        });\n        \n        await holisticCamera.start();\n        log.innerText = \"Detección completa activa - Muestra tus manos y rostro\";\n        \n    } catch (e) {\n        log.innerText = \"Error: \" + e.message;\n        console.error(e);\n    }\n}\n</script>",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<div style=\"text-align: center; background: #1a1a1a; padding: 20px; border-radius: 15px; color: white; font-family: sans-serif;\">\n    <h3>MediaPipe</h3>\n    <video id=\"v_pose_pro\" width=\"640\" height=\"480\" style=\"display:none\" playsinline></video>\n    <canvas id=\"c_pose_pro\" width=\"640\" height=\"480\" style=\"background: #000; border: 2px solid #9b59b6; border-radius: 10px;\"></canvas>\n\n    <div style=\"margin-top: 15px;\">\n        <button id=\"btn_on_pose\" onclick=\"iniciarPosePro()\" style=\"padding: 10px 20px; background: #9b59b6; color: white; border: none; border-radius: 5px; cursor: pointer; font-weight: bold;\">ACTIVAR DETECCIÓN</button>\n        <button id=\"btn_off_pose\" onclick=\"detenerTodo()\" style=\"padding: 10px 20px; background: #e74c3c; color: white; border: none; border-radius: 5px; cursor: pointer; margin-left: 10px; display: none;\">APAGAR</button>\n    </div>\n\n    <div style=\"margin-top: 10px; display: flex; justify-content: center; gap: 5px; flex-wrap: wrap;\">\n        <button onclick=\"togglePart('body')\" style=\"padding: 8px 15px; background: #3498db; color: white; border: none; border-radius: 5px; cursor: pointer;\">Cuerpo</button>\n        <button onclick=\"togglePart('hands')\" style=\"padding: 8px 15px; background: #f39c12; color: white; border: none; border-radius: 5px; cursor: pointer;\">Manos (21 puntos)</button>\n        <button onclick=\"togglePart('face')\" style=\"padding: 8px 15px; background: #e74c3c; color: white; border: none; border-radius: 5px; cursor: pointer;\">Rostro (468 puntos)</button>\n    </div>\n\n    <p id=\"log_pose_pro\" style=\"color: #f1c40f; font-size: 13px; margin-top: 10px;\">Estado: Esperando cámara...</p>\n    <p style=\"color: #95a5a6; font-size: 11px; margin-top: 5px;\">Tip: Acerca tu mano o rostro para ver detalle máximo</p>\n</div>\n\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/holistic/holistic.js\" crossorigin=\"anonymous\"></script>\n\n<script>\n// Variables de configuración\nlet visibleParts = {\n    body: true,\n    hands: true,\n    face: true\n};\n\nlet holisticCamera = null;\n\nfunction togglePart(part) {\n    visibleParts[part] = !visibleParts[part];\n}\n\nasync function iniciarPosePro() {\n    const log = document.getElementById('log_pose_pro');\n\n    detenerTodo();\n\n    ['c_canny_pro', 'c_out', 'c_sobel_pro', 'c_pose_pro'].forEach(canvasId => {\n        const canvas = document.getElementById(canvasId);\n        if (canvas) {\n            const ctx = canvas.getContext('2d');\n            ctx.clearRect(0, 0, canvas.width, canvas.height);\n            ctx.fillStyle = '#000';\n            ctx.fillRect(0, 0, canvas.width, canvas.height);\n        }\n    });\n\n    try {\n        log.innerText = \"Solicitando cámara...\";\n\n        const stream = await navigator.mediaDevices.getUserMedia({ \n            video: { width: 640, height: 480, facingMode: 'user' } \n        });\n\n        window.cameraManager.currentStream = stream;\n        window.cameraManager.currentFilter = 'pose';\n\n        const v = document.getElementById('v_pose_pro');\n        const canvas = document.getElementById('c_pose_pro');\n        const ctx = canvas.getContext('2d');\n\n        v.srcObject = stream;\n        await v.play();\n\n        document.getElementById('btn_on_pose').style.display = 'none';\n        document.getElementById('btn_off_pose').style.display = 'inline-block';\n\n        log.innerText = \"Cargando MediaPipe Holistic (cuerpo completo + manos + rostro)...\";\n\n        // Usar HOLISTIC para obtener TODO con alta precisión\n        const holistic = new Holistic({\n            locateFile: (file) => {\n                return `https://cdn.jsdelivr.net/npm/@mediapipe/holistic/${file}`;\n            }\n        });\n\n        holistic.setOptions({\n            modelComplexity: 2,\n            smoothLandmarks: true,\n            enableSegmentation: false,\n            smoothSegmentation: false,\n            refineFaceLandmarks: true, // ¡CLAVE! Rostro detallado\n            minDetectionConfidence: 0.5,\n            minTrackingConfidence: 0.5\n        });\n\n        let frameCount = 0;\n\n        holistic.onResults((results) => {\n            if (window.cameraManager.currentFilter !== 'pose') return;\n\n            ctx.save();\n            ctx.clearRect(0, 0, canvas.width, canvas.height);\n            ctx.drawImage(results.image, 0, 0, canvas.width, canvas.height);\n\n            let detections = [];\n\n            // ========== CUERPO (33 puntos) ==========\n            if (visibleParts.body && results.poseLandmarks) {\n                // Esqueleto del cuerpo\n                drawConnectors(ctx, results.poseLandmarks, POSE_CONNECTIONS, {\n                    color: '#00FFFF',\n                    lineWidth: 4\n                });\n\n                // Puntos del cuerpo\n                drawLandmarks(ctx, results.poseLandmarks, {\n                    color: '#00FFFF',\n                    fillColor: '#00FFFF',\n                    lineWidth: 2,\n                    radius: 5\n                });\n\n                detections.push('Cuerpo (33 puntos)');\n            }\n\n            // ========== MANOS IZQUIERDA (21 puntos cada una) ==========\n            if (visibleParts.hands) {\n                // Mano izquierda\n                if (results.leftHandLandmarks) {\n                    drawConnectors(ctx, results.leftHandLandmarks, HAND_CONNECTIONS, {\n                        color: '#FFD700',\n                        lineWidth: 3\n                    });\n                    drawLandmarks(ctx, results.leftHandLandmarks, {\n                        color: '#FFD700',\n                        fillColor: '#FFFF00',\n                        lineWidth: 2,\n                        radius: 4\n                    });\n                    detections.push('Mano Izq (21 puntos)');\n                }\n\n                // Mano derecha\n                if (results.rightHandLandmarks) {\n                    drawConnectors(ctx, results.rightHandLandmarks, HAND_CONNECTIONS, {\n                        color: '#FF8C00',\n                        lineWidth: 3\n                    });\n                    drawLandmarks(ctx, results.rightHandLandmarks, {\n                        color: '#FF8C00',\n                        fillColor: '#FFA500',\n                        lineWidth: 2,\n                        radius: 4\n                    });\n                    detections.push('Mano Der (21 puntos)');\n                }\n            }\n\n            // ========== ROSTRO (468 puntos - ¡SÍ, 468!) ==========\n            if (visibleParts.face && results.faceLandmarks) {\n                // Malla facial completa\n                drawConnectors(ctx, results.faceLandmarks, FACEMESH_TESSELATION, {\n                    color: '#00FF00',\n                    lineWidth: 0.5\n                });\n\n                // Contornos importantes del rostro\n                drawConnectors(ctx, results.faceLandmarks, FACEMESH_RIGHT_EYE, {\n                    color: '#00FF00',\n                    lineWidth: 2\n                });\n                drawConnectors(ctx, results.faceLandmarks, FACEMESH_LEFT_EYE, {\n                    color: '#00FF00',\n                    lineWidth: 2\n                });\n                drawConnectors(ctx, results.faceLandmarks, FACEMESH_LIPS, {\n                    color: '#FF0000',\n                    lineWidth: 2\n                });\n                drawConnectors(ctx, results.faceLandmarks, FACEMESH_FACE_OVAL, {\n                    color: '#E0E0E0',\n                    lineWidth: 2\n                });\n\n                // Puntos clave del rostro\n                drawLandmarks(ctx, results.faceLandmarks, {\n                    color: '#FF1493',\n                    fillColor: '#FF69B4',\n                    lineWidth: 1,\n                    radius: 1\n                });\n\n                detections.push('Rostro (468 puntos)');\n            }\n\n            frameCount++;\n            if (frameCount % 30 === 0) {\n                const detectedText = detections.length > 0 ? detections.join(' + ') : 'Esperando...';\n                log.innerText = `✅ ${detectedText} | Frames: ${frameCount}`;\n            }\n\n            ctx.restore();\n        });\n\n        // Iniciar cámara\n        holisticCamera = new Camera(v, {\n            onFrame: async () => {\n                if (window.cameraManager.currentFilter === 'pose') {\n                    await holistic.send({image: v});\n                }\n            },\n            width: 640,\n            height: 480\n        });\n\n        await holisticCamera.start();\n        log.innerText = \"Detección completa activa - Muestra tus manos y rostro\";\n\n    } catch (e) {\n        log.innerText = \"Error: \" + e.message;\n        console.error(e);\n    }\n}\n</script>\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11
    },
    {
      "id": "799ef529-e578-4036-985a-1d70648bc0e0",
      "cell_type": "markdown",
      "source": "---\n\n### 19. ¿Qué son las redes neuronales convolucionales (CNN)?\n[cite_start]Son un tipo de arquitectura de Deep Learning diseñada específicamente para procesar datos con estructura de cuadrícula, como las imágenes[cite: 160]. [cite_start]\n\n[Image of convolutional neural network architecture]\n Utilizan capas de \"convolución\" que funcionan como filtros aprendidos automáticamente[cite: 161]. [cite_start]A diferencia de Sobel (donde el humano define el filtro), una CNN aprende por sí sola qué filtros aplicar para detectar desde líneas simples hasta formas complejas como una cara humana[cite: 161].\n\n### 23. Escribir el mismo concepto pero usando Sobel\n[cite_start]Implementación de procesamiento de video en tiempo real utilizando el operador Sobel para detección de bordes mediante OpenCV.js[cite: 163].",
      "metadata": {}
    },
    {
      "id": "f07f344a-98be-4d8c-8988-2c433dc390c6",
      "cell_type": "markdown",
      "source": "# Análisis Matemático: Implementación Manual del Operador Sobel\n\nEn este módulo, nuestro equipo ha programado el algoritmo de Sobel manipulando directamente los buffers de memoria de la imagen. El objetivo es calcular el **Gradiente de Intensidad** de la imagen, que es un vector que apunta en la dirección del mayor cambio de brillo (el borde).\n\nA continuación, describimos las operaciones algebraicas que realizamos paso a paso en el código:\n\n### 1. Reducción Dimensional (Escala de Grises)\nLa primera etapa consiste en transformar el espacio de color tridimensional ($R, G, B$) en un campo escalar bidimensional $I(x,y)$. Utilizamos la combinación lineal estándar para la luminancia:\n\n$$\nI(x,y) = 0.299 \\cdot R + 0.587 \\cdot G + 0.114 \\cdot B\n$$\n\n> **En nuestro código:** Iteramos por el array `data` leyendo de 4 en 4 (RGBA) y guardamos el resultado en el array `gray`.\n\n---\n\n### 2. Derivación Discreta (Máscaras de Convolución)\nPara encontrar los bordes, necesitamos calcular la derivada de la intensidad. Como la imagen es discreta, utilizamos **Diferencias Finitas Centrales** ponderadas.\n\nImplementamos dos núcleos (kernels) de convolución de $3 \\times 3$. En el bucle `for` anidado, para cada píxel $(x,y)$, operamos sobre su **Vecindad de 8-conexión**:\n\n#### A. Gradiente Horizontal ($G_x$)\nCalcula la diferencia de intensidad de izquierda a derecha. Nuestro código aplica explícitamente la siguiente matriz:\n\n$$\nG_x = \\begin{bmatrix} \n-1 & 0 & +1 \\\\ \n-2 & 0 & +2 \\\\ \n-1 & 0 & +1 \n\\end{bmatrix} * I\n$$\n\n> **Lógica del código:** `(gray[derecha] - gray[izquierda])`. Los píxeles centrales tienen peso 2 para suavizar el ruido local.\n\n#### B. Gradiente Vertical ($G_y$)\nCalcula la diferencia de intensidad de arriba a abajo.\n\n$$\nG_y = \\begin{bmatrix} \n-1 & -2 & -1 \\\\ \n0 & 0 & 0 \\\\ \n+1 & +2 & +1 \n\\end{bmatrix} * I\n$$\n\n> **Lógica del código:** `(gray[abajo] - gray[arriba])`.\n\n---\n\n### 3. Cálculo de la Magnitud (Norma Euclidiana)\nUna vez obtenidos los componentes vectoriales $G_x$ y $G_y$, necesitamos la intensidad total del borde.\nEn esta implementación, hemos optado por la precisión matemática utilizando la **Norma Euclidiana ($L_2$)**:\n\n$$\n|G| = \\sqrt{G_x^2 + G_y^2}\n$$\n\n> **En nuestro código:** La línea `const mag = Math.sqrt(gx * gx + gy * gy)` realiza este cálculo pitagórico. Finalmente, limitamos el valor a 255 (`Math.min`) para visualizarlo correctamente en el monitor.\n\n### Conclusión del Algoritmo\nEl resultado visual es un **Mapa de Magnitudes**: las zonas planas (color negro) tienen derivada cercana a cero, mientras que los cambios bruscos (bordes) tienen derivadas altas, apareciendo en blanco.",
      "metadata": {}
    },
    {
      "id": "301c4af0-d3da-4be4-a19f-ed6b4ce79c53",
      "cell_type": "code",
      "source": "%%html\n<div style=\"text-align: center; background: #1a1a1a; padding: 20px; border-radius: 15px; color: white; font-family: sans-serif;\">\n    <h3>Método Sobel (Gradientes de Intensidad)</h3>\n    <video id=\"v_sobel_pro\" width=\"640\" height=\"480\" style=\"display:none\" playsinline></video>\n    <canvas id=\"c_sobel_pro\" width=\"640\" height=\"480\" style=\"background: #000; border: 2px solid #2980b9; border-radius: 10px;\"></canvas>\n    <div style=\"margin-top: 15px;\">\n        <button id=\"btn_on_sobel\" onclick=\"iniciarSobelPro()\" style=\"padding: 10px 20px; background: #2980b9; color: white; border: none; border-radius: 5px; cursor: pointer; font-weight: bold;\">ACTIVAR SOBEL</button>\n        <button id=\"btn_off_sobel\" onclick=\"detenerTodo()\" style=\"padding: 10px 20px; background: #e74c3c; color: white; border: none; border-radius: 5px; cursor: pointer; margin-left: 10px; display: none;\">APAGAR</button>\n    </div>\n    <p id=\"log_sobel_pro\" style=\"color: #f1c40f; font-size: 13px; margin-top: 10px;\">Estado: Esperando cámara...</p>\n</div>\n\n<script>\n// Implementación de Sobel\nfunction applySobelGradients(imageData) {\n    const width = imageData.width;\n    const height = imageData.height;\n    const data = imageData.data;\n    \n    // Convertir a escala de grises\n    const gray = new Uint8ClampedArray(width * height);\n    for (let i = 0; i < data.length; i += 4) {\n        const idx = i / 4;\n        gray[idx] = 0.299 * data[i] + 0.587 * data[i + 1] + 0.114 * data[i + 2];\n    }\n    \n    // Aplicar Sobel\n    const gradX = new Float32Array(width * height);\n    const gradY = new Float32Array(width * height);\n    const magnitude = new Uint8ClampedArray(width * height);\n    \n    for (let y = 1; y < height - 1; y++) {\n        for (let x = 1; x < width - 1; x++) {\n            const idx = y * width + x;\n            \n            // Sobel X (detecta bordes verticales)\n            const gx = (\n                -gray[(y-1)*width + (x-1)] + gray[(y-1)*width + (x+1)] +\n                -2*gray[y*width + (x-1)] + 2*gray[y*width + (x+1)] +\n                -gray[(y+1)*width + (x-1)] + gray[(y+1)*width + (x+1)]\n            );\n            \n            // Sobel Y (detecta bordes horizontales)\n            const gy = (\n                -gray[(y-1)*width + (x-1)] - 2*gray[(y-1)*width + x] - gray[(y-1)*width + (x+1)] +\n                gray[(y+1)*width + (x-1)] + 2*gray[(y+1)*width + x] + gray[(y+1)*width + (x+1)]\n            );\n            \n            gradX[idx] = gx;\n            gradY[idx] = gy;\n            \n            // Magnitud del gradiente\n            const mag = Math.sqrt(gx * gx + gy * gy);\n            magnitude[idx] = Math.min(255, mag);\n        }\n    }\n    \n    // Convertir a ImageData\n    const output = new ImageData(width, height);\n    for (let i = 0; i < magnitude.length; i++) {\n        output.data[i * 4] = magnitude[i];\n        output.data[i * 4 + 1] = magnitude[i];\n        output.data[i * 4 + 2] = magnitude[i];\n        output.data[i * 4 + 3] = 255;\n    }\n    \n    return output;\n}\n\nasync function iniciarSobelPro() {\n    const log = document.getElementById('log_sobel_pro');\n    \n    // Detener otros filtros Y limpiar sus canvas\n    detenerTodo();\n    \n    // IMPORTANTE: Limpiar TODOS los canvas antes de empezar\n    ['c_canny_pro', 'c_out', 'c_sobel_pro'].forEach(canvasId => {\n        const canvas = document.getElementById(canvasId);\n        if (canvas) {\n            const ctx = canvas.getContext('2d');\n            ctx.clearRect(0, 0, canvas.width, canvas.height);\n            ctx.fillStyle = '#000';\n            ctx.fillRect(0, 0, canvas.width, canvas.height);\n        }\n    });\n    \n    try {\n        log.innerText = \"📷 Solicitando cámara...\";\n        \n        const stream = await navigator.mediaDevices.getUserMedia({ \n            video: { width: 640, height: 480, facingMode: 'user' } \n        });\n        \n        window.cameraManager.currentStream = stream;\n        window.cameraManager.currentFilter = 'sobel';\n        \n        const v = document.getElementById('v_sobel_pro');\n        const canvas = document.getElementById('c_sobel_pro');\n        const ctx = canvas.getContext('2d');\n        \n        v.srcObject = stream;\n        await v.play();\n        \n        await new Promise(resolve => setTimeout(resolve, 500));\n        \n        document.getElementById('btn_on_sobel').style.display = 'none';\n        document.getElementById('btn_off_sobel').style.display = 'inline-block';\n        log.innerText = \"Gradientes Sobel activos\";\n        \n        window.cameraManager.activeLoop = true;\n        let frameCount = 0;\n        \n        function processFrame() {\n            if (!window.cameraManager.activeLoop || window.cameraManager.currentFilter !== 'sobel') {\n                return;\n            }\n            \n            try {\n                ctx.drawImage(v, 0, 0, canvas.width, canvas.height);\n                const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);\n                \n                // Aplicar SOBEL\n                const edges = applySobelGradients(imageData);\n                \n                ctx.putImageData(edges, 0, 0);\n                \n                frameCount++;\n                if (frameCount % 30 === 0) {\n                    log.innerText = \"Sobel activo - Frames: \" + frameCount;\n                }\n                \n                requestAnimationFrame(processFrame);\n                \n            } catch (e) {\n                console.error('Error:', e);\n                log.innerText = \"Error: \" + e.message;\n            }\n        }\n        \n        processFrame();\n        \n    } catch (e) {\n        log.innerText = \"Error: \" + e.message;\n        console.error(e);\n    }\n}\n</script>",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<div style=\"text-align: center; background: #1a1a1a; padding: 20px; border-radius: 15px; color: white; font-family: sans-serif;\">\n    <h3>Método Sobel (Gradientes de Intensidad)</h3>\n    <video id=\"v_sobel_pro\" width=\"640\" height=\"480\" style=\"display:none\" playsinline></video>\n    <canvas id=\"c_sobel_pro\" width=\"640\" height=\"480\" style=\"background: #000; border: 2px solid #2980b9; border-radius: 10px;\"></canvas>\n    <div style=\"margin-top: 15px;\">\n        <button id=\"btn_on_sobel\" onclick=\"iniciarSobelPro()\" style=\"padding: 10px 20px; background: #2980b9; color: white; border: none; border-radius: 5px; cursor: pointer; font-weight: bold;\">ACTIVAR SOBEL</button>\n        <button id=\"btn_off_sobel\" onclick=\"detenerTodo()\" style=\"padding: 10px 20px; background: #e74c3c; color: white; border: none; border-radius: 5px; cursor: pointer; margin-left: 10px; display: none;\">APAGAR</button>\n    </div>\n    <p id=\"log_sobel_pro\" style=\"color: #f1c40f; font-size: 13px; margin-top: 10px;\">Estado: Esperando cámara...</p>\n</div>\n\n<script>\n// Implementación de Sobel\nfunction applySobelGradients(imageData) {\n    const width = imageData.width;\n    const height = imageData.height;\n    const data = imageData.data;\n\n    // Convertir a escala de grises\n    const gray = new Uint8ClampedArray(width * height);\n    for (let i = 0; i < data.length; i += 4) {\n        const idx = i / 4;\n        gray[idx] = 0.299 * data[i] + 0.587 * data[i + 1] + 0.114 * data[i + 2];\n    }\n\n    // Aplicar Sobel\n    const gradX = new Float32Array(width * height);\n    const gradY = new Float32Array(width * height);\n    const magnitude = new Uint8ClampedArray(width * height);\n\n    for (let y = 1; y < height - 1; y++) {\n        for (let x = 1; x < width - 1; x++) {\n            const idx = y * width + x;\n\n            // Sobel X (detecta bordes verticales)\n            const gx = (\n                -gray[(y-1)*width + (x-1)] + gray[(y-1)*width + (x+1)] +\n                -2*gray[y*width + (x-1)] + 2*gray[y*width + (x+1)] +\n                -gray[(y+1)*width + (x-1)] + gray[(y+1)*width + (x+1)]\n            );\n\n            // Sobel Y (detecta bordes horizontales)\n            const gy = (\n                -gray[(y-1)*width + (x-1)] - 2*gray[(y-1)*width + x] - gray[(y-1)*width + (x+1)] +\n                gray[(y+1)*width + (x-1)] + 2*gray[(y+1)*width + x] + gray[(y+1)*width + (x+1)]\n            );\n\n            gradX[idx] = gx;\n            gradY[idx] = gy;\n\n            // Magnitud del gradiente\n            const mag = Math.sqrt(gx * gx + gy * gy);\n            magnitude[idx] = Math.min(255, mag);\n        }\n    }\n\n    // Convertir a ImageData\n    const output = new ImageData(width, height);\n    for (let i = 0; i < magnitude.length; i++) {\n        output.data[i * 4] = magnitude[i];\n        output.data[i * 4 + 1] = magnitude[i];\n        output.data[i * 4 + 2] = magnitude[i];\n        output.data[i * 4 + 3] = 255;\n    }\n\n    return output;\n}\n\nasync function iniciarSobelPro() {\n    const log = document.getElementById('log_sobel_pro');\n\n    // Detener otros filtros Y limpiar sus canvas\n    detenerTodo();\n\n    // IMPORTANTE: Limpiar TODOS los canvas antes de empezar\n    ['c_canny_pro', 'c_out', 'c_sobel_pro'].forEach(canvasId => {\n        const canvas = document.getElementById(canvasId);\n        if (canvas) {\n            const ctx = canvas.getContext('2d');\n            ctx.clearRect(0, 0, canvas.width, canvas.height);\n            ctx.fillStyle = '#000';\n            ctx.fillRect(0, 0, canvas.width, canvas.height);\n        }\n    });\n\n    try {\n        log.innerText = \"📷 Solicitando cámara...\";\n\n        const stream = await navigator.mediaDevices.getUserMedia({ \n            video: { width: 640, height: 480, facingMode: 'user' } \n        });\n\n        window.cameraManager.currentStream = stream;\n        window.cameraManager.currentFilter = 'sobel';\n\n        const v = document.getElementById('v_sobel_pro');\n        const canvas = document.getElementById('c_sobel_pro');\n        const ctx = canvas.getContext('2d');\n\n        v.srcObject = stream;\n        await v.play();\n\n        await new Promise(resolve => setTimeout(resolve, 500));\n\n        document.getElementById('btn_on_sobel').style.display = 'none';\n        document.getElementById('btn_off_sobel').style.display = 'inline-block';\n        log.innerText = \"Gradientes Sobel activos\";\n\n        window.cameraManager.activeLoop = true;\n        let frameCount = 0;\n\n        function processFrame() {\n            if (!window.cameraManager.activeLoop || window.cameraManager.currentFilter !== 'sobel') {\n                return;\n            }\n\n            try {\n                ctx.drawImage(v, 0, 0, canvas.width, canvas.height);\n                const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);\n\n                // Aplicar SOBEL\n                const edges = applySobelGradients(imageData);\n\n                ctx.putImageData(edges, 0, 0);\n\n                frameCount++;\n                if (frameCount % 30 === 0) {\n                    log.innerText = \"Sobel activo - Frames: \" + frameCount;\n                }\n\n                requestAnimationFrame(processFrame);\n\n            } catch (e) {\n                console.error('Error:', e);\n                log.innerText = \"Error: \" + e.message;\n            }\n        }\n\n        processFrame();\n\n    } catch (e) {\n        log.innerText = \"Error: \" + e.message;\n        console.error(e);\n    }\n}\n</script>\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19
    },
    {
      "id": "6601b4a4-9566-455a-8583-3a9e2dc1d60b",
      "cell_type": "markdown",
      "source": "### Codigo Three.js\n\n# Introducción a Three.js + Face Mesh (AR)\n\nEste proyecto es un ejemplo práctico de **Realidad Aumentada (AR)** en la web. Utiliza la potencia de los gráficos 3D junto con inteligencia artificial para crear una experiencia interactiva en tiempo real.\n\n---\n\n## 1. El Matrimonio de dos Tecnologías\nEl código funciona gracias a la colaboración de dos herramientas poderosas:\n\n* **Three.js:** Se encarga de la **capa visual**. Crea la esfera amarilla, gestiona las luces, la cámara y renderiza los gráficos a alta velocidad directamente en el navegador.\n* **MediaPipe (Face Mesh):** Es el **cerebro de IA** desarrollado por Google. Analiza el flujo de video de tu cámara para identificar 468 puntos clave de tu rostro en milisegundos.\n\n\n\n---\n\n## 2. El Flujo Lógico del Proceso\nPara que la esfera \"persiga\" tu nariz, el código ejecuta un ciclo continuo de cuatro pasos:\n\n1.  **Captura:** Se solicita acceso a la webcam y se genera un elemento de `<video>` que sirve como fuente de datos.\n2.  **Detección (IA):** MediaPipe recibe cada fotograma del video, localiza el punto específico de tu nariz y genera coordenadas matemáticas $x, y, z$.\n3.  **Traducción:** El script convierte esas coordenadas del mundo real (píxeles de la cámara) a las coordenadas del mundo 3D de la escena de Three.js.\n4.  **Renderizado:** Three.js redibuja la esfera en la nueva posición aproximadamente **60 veces por segundo** (60 FPS), creando una sensación de movimiento fluido y natural.\n\n---\n\n## 3. Anatomía del Objeto 3D\nDentro de la función `crearEsferaGuia()`, definimos el \"ADN\" del objeto usando dos componentes base:\n\n| Componente | Definición en Código | Descripción |\n| :--- | :--- | :--- |\n| **Geometría** | `IcosahedronGeometry(30, 2)` | Define la estructura física. Es un icosaedro con subdivisiones que le dan forma de esfera facetada. |\n| **Material** | `MeshBasicMaterial` | Define la apariencia. El color `0xffff00` (amarillo) y la propiedad `wireframe: true` le dan ese estilo de \"malla\" o holograma. |\n\n\n\n---\n\n## 4. ¿Por qué es especial este código?\nA diferencia de una animación 3D tradicional, este sistema es **contextual e interactivo**:\n\n* **Capas Superpuestas:** Utiliza un `<canvas>` transparente colocado exactamente encima del video de la cámara.\n* **Cámara Ortográfica:** Se usa `OrthographicCamera` para evitar que el objeto se deforme por la perspectiva, facilitando que la esfera encaje perfectamente con tu rostro en una pantalla 2D.\n* **Eficiencia:** Todo el procesamiento ocurre en el dispositivo del usuario (Client-side), lo que garantiza privacidad y rapidez sin depender de un servidor externo.\n\n---\n> **Nota técnica:** Este flujo es la base para crear filtros de Instagram o aplicaciones de probadores virtuales (gafas, sombreros, maquillaje) directamente en la web.",
      "metadata": {}
    },
    {
      "id": "8d35d7f0-2d6c-473f-95ed-8a26b5d1c860",
      "cell_type": "markdown",
      "source": "# Análisis Matemático: Integración de Gráficos 3D con Three.js\n\nEn esta sección, nuestro objetivo era renderizar objetos tridimensionales interactivos que respondieran a la geometría del rostro en tiempo real. Para ello, utilizamos la librería **Three.js**, que nos permite manipular vértices y matrices en un entorno WebGL.\n\nEl desafío principal fue la **Sincronización de Coordenadas**: MediaPipe entrega coordenadas normalizadas (2D+Z relativo), mientras que Three.js trabaja en un espacio métrico euclidiano (3D absoluto).\n\nA continuación, detallamos la lógica matemática de la integración:\n\n### 1. Configuración de la Escena (Proyección Ortográfica)\nDado que estamos trabajando sobre un video 2D plano, utilizamos una **Cámara Ortográfica** en lugar de una de Perspectiva.\nMatemáticamente, esto elimina la deformación por punto de fuga, garantizando que el tamaño de los objetos dependa exclusivamente de nuestra lógica de escala y no de la distancia virtual de la cámara 3D.\n\n$$\nP_{proyectado} = \\begin{bmatrix} \n\\frac{2}{r-l} & 0 & 0 & -\\frac{r+l}{r-l} \\\\\n0 & \\frac{2}{t-b} & 0 & -\\frac{t+b}{t-b} \\\\\n0 & 0 & -\\frac{2}{f-n} & -\\frac{f+n}{f-n} \\\\\n0 & 0 & 0 & 1 \n\\end{bmatrix} \\cdot P_{mundo}\n$$\n\n> **En nuestro código:** Configuramos los límites `left=-320`, `right=320`, `top=240`, `bottom=-240` para que coincidan píxel a píxel con la resolución del video ($640 \\times 480$).\n\n### 2. Mapeo de Coordenadas (Transformación Afín)\nEl núcleo del algoritmo es la función que traduce la posición de la nariz (Landmark #4) al espacio 3D de Three.js.\n\n1.  **Centrado del Eje:** MediaPipe tiene su origen $(0,0)$ en la esquina superior izquierda. Three.js tiene su origen $(0,0)$ en el centro exacto de la pantalla.\n    * **Ecuación de traslación X:** $x_{3D} = (x_{MP} - 0.5) \\cdot Ancho$\n    * **Ecuación de traslación Y:** $y_{3D} = -(y_{MP} - 0.5) \\cdot Alto$ (Invertimos el signo porque en 3D el eje Y positivo suele ir hacia arriba).\n\n2.  **Profundidad (Eje Z):** MediaPipe entrega un valor Z relativo a la escala de la cara. Lo multiplicamos por un factor escalar empírico ($-200$) para que la esfera parezca acercarse o alejarse correctamente.\n\n### 3. Renderizado de la Esfera (Geometría Icosaédrica)\nPara la \"Esfera Guía\", utilizamos un **Icosaedro** (`IcosahedronGeometry`) subdividido 2 veces.\n* **Topología:** Es un poliedro convexo regular. Al activar `wireframe: true`, el motor gráfico solo renderiza las aristas del grafo poliedral, permitiendo ver a través del objeto y apreciar su rotación.\n* **Animación:** En cada frame de renderizado, aplicamos una matriz de rotación incremental:\n    $$\n    R_{total} = R_y(\\theta + 0.02) \\cdot R_x(\\phi + 0.01)\n    $$\n    Esto genera un giro constante que ayuda a visualizar la tridimensionalidad del objeto sobre el video plano.",
      "metadata": {}
    },
    {
      "id": "63613df1-7e57-42ba-93d5-e2173ed140ae",
      "cell_type": "code",
      "source": "%%html\n<div style=\"text-align: center; background: #1a1a1a; padding: 20px; border-radius: 15px; color: white; font-family: sans-serif;\">\n    <h3>Three AR (Esfera Guía)</h3>\n    <p style=\"font-size: 12px; color: #aaa;\">La esfera te esta siguiendo</p>\n    \n    <div id=\"threejs_container_sphere\" style=\"position: relative; display: inline-block; width: 640px; height: 480px; border: 2px solid #555; border-radius: 10px; overflow: hidden; background: #000;\">\n        </div>\n    \n    <div style=\"margin-top: 15px;\">\n        <button id=\"btn_on_sphere\" onclick=\"iniciarSphereAR()\" style=\"padding: 10px 20px; background: #f1c40f; color: #000; border: none; border-radius: 5px; cursor: pointer; font-weight: bold;\">ACTIVAR ESFERA</button>\n        <button id=\"btn_off_sphere\" onclick=\"detenerSphereAR()\" style=\"padding: 10px 20px; background: #e74c3c; color: white; border: none; border-radius: 5px; cursor: pointer; margin-left: 10px; display: none;\">APAGAR</button>\n    </div>\n\n    \n    <div style=\"margin-top: 10px;\">\n        <button id=\"toggle_bg_sphere\" onclick=\"toggleBackgroundSphere()\" style=\"padding: 8px 15px; background: #34495e; color: white; border: none; border-radius: 5px; cursor: pointer; font-size: 12px;\">Activar camara</button>\n    </div>\n    \n    <p id=\"log_sphere\" style=\"color: #f1c40f; font-size: 13px; margin-top: 10px;\">Estado: Esperando cámara...</p>\n</div>\n\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js\" crossorigin=\"anonymous\"></script>\n\n<script>\nlet tScene, tCamera, tRenderer, tObject;\nlet tCameraInstance = null;\nlet tVideoElement = null;\nlet isVideoVisible = false; // Estado inicial: Video oculto\n\n// --- FUNCIÓN CORREGIDA PARA MOSTRAR/OCULTAR VIDEO ---\nfunction toggleBackgroundSphere() {\n    // 1. Buscamos el elemento de video directamente por su ID para asegurar que existe\n    const vid = document.getElementById('v_threejs_sphere');\n    const btn = document.getElementById('toggle_bg_sphere');\n    \n    if (vid) {\n        isVideoVisible = !isVideoVisible;\n        // Cambiamos el estilo display entre 'block' (visible) y 'none' (oculto)\n        vid.style.display = isVideoVisible ? 'block' : 'none';\n        // Actualizamos el texto del botón\n        if (btn) btn.innerText = isVideoVisible ? 'Ocultar Mi Cara' : 'Mostrar Mi Cara';\n    } else {\n        console.log(\"El video aún no está listo. Inicia la cámara primero.\");\n    }\n}\n\n// Función para crear la esfera amarilla de alambre (igual que tu imagen)\nfunction crearEsferaGuia() {\n    const geometry = new THREE.IcosahedronGeometry(30, 2); // Esfera geodésica\n    const material = new THREE.MeshBasicMaterial({ \n        color: 0xffff00,       // Amarillo\n        wireframe: true        // Modo alambre\n    });\n    return new THREE.Mesh(geometry, material);\n}\n\n// Limpieza de memoria\nfunction detenerSphereAR() {\n    if (tCameraInstance) {\n        try { tCameraInstance.stop(); } catch(e) { console.log(e); }\n        tCameraInstance = null;\n    }\n    \n    if (tRenderer) { tRenderer.dispose(); tRenderer = null; }\n    if (tScene) {\n        while(tScene.children.length > 0) { tScene.remove(tScene.children[0]); }\n        tScene = null;\n    }\n    \n    if (window.cameraManager && window.cameraManager.currentStream) {\n        window.cameraManager.currentStream.getTracks().forEach(t => t.stop());\n        window.cameraManager.currentStream = null;\n    }\n    \n    if (tVideoElement) {\n        if (tVideoElement.srcObject) {\n            tVideoElement.srcObject.getTracks().forEach(t => t.stop());\n            tVideoElement.srcObject = null;\n        }\n        tVideoElement = null;\n    }\n    \n    // Reseteamos estado del botón\n    isVideoVisible = false;\n    const btn = document.getElementById('toggle_bg_sphere');\n    if(btn) btn.innerText = 'Mostrar Mi Cara';\n    \n    const container = document.getElementById('threejs_container_sphere');\n    if (container) container.innerHTML = '';\n    \n    document.getElementById('btn_on_sphere').style.display = 'inline-block';\n    document.getElementById('btn_off_sphere').style.display = 'none';\n    document.getElementById('log_sphere').innerText = \"Sistema apagado.\";\n}\n\nasync function iniciarSphereAR() {\n    const log = document.getElementById('log_sphere');\n    \n    try {\n        log.innerText = \"Iniciando sistema...\";\n        detenerSphereAR(); // Limpieza previa\n        await new Promise(resolve => setTimeout(resolve, 300));\n        \n        const container = document.getElementById('threejs_container_sphere');\n        \n        // 1. CREAMOS EL VIDEO (Oculto por defecto con display: none)\n        tVideoElement = document.createElement('video');\n        tVideoElement.id = 'v_threejs_sphere'; // ID importante para el botón\n        tVideoElement.width = 640;\n        tVideoElement.height = 480;\n        tVideoElement.autoplay = true;\n        tVideoElement.playsInline = true;\n        tVideoElement.muted = true;\n        // style.display = none inicial\n        tVideoElement.style.cssText = 'position: absolute; top: 0; left: 0; transform: scaleX(-1); width: 100%; height: 100%; display: none;';\n        container.appendChild(tVideoElement);\n        \n        // 2. CREAMOS EL CANVAS (Donde se dibuja la esfera)\n        const tCanvas = document.createElement('canvas');\n        tCanvas.id = 'c_threejs_sphere';\n        tCanvas.width = 640;\n        tCanvas.height = 480;\n        // z-index alto para estar encima\n        tCanvas.style.cssText = 'position: absolute; top: 0; left: 0; transform: scaleX(-1); width: 100%; height: 100%; z-index: 10;';\n        container.appendChild(tCanvas);\n        \n        log.innerText = \"Accediendo a cámara...\";\n        \n        const stream = await navigator.mediaDevices.getUserMedia({ \n            video: { width: 640, height: 480, facingMode: 'user' } \n        });\n        \n        if (!window.cameraManager) window.cameraManager = {};\n        window.cameraManager.currentStream = stream;\n        window.cameraManager.currentFilter = 'sphere_ar';\n        \n        tVideoElement.srcObject = stream;\n        await tVideoElement.play();\n        \n        await new Promise(resolve => {\n            if (tVideoElement.videoWidth > 0) resolve();\n            else tVideoElement.onloadedmetadata = () => resolve();\n        });\n        \n        document.getElementById('btn_on_sphere').style.display = 'none';\n        document.getElementById('btn_off_sphere').style.display = 'inline-block';\n        \n        log.innerText = \"Configurando 3D...\";\n        \n        // ========== CONFIGURAR THREE.JS ==========\n        tScene = new THREE.Scene();\n        tCamera = new THREE.OrthographicCamera(-320, 320, 240, -240, 0.1, 1000);\n        tCamera.position.z = 10;\n        \n        tRenderer = new THREE.WebGLRenderer({ \n            canvas: tCanvas, \n            alpha: true, // Fondo transparente\n            antialias: true \n        });\n        tRenderer.setSize(640, 480);\n        \n        // Agregar la Esfera Amarilla\n        tObject = crearEsferaGuia();\n        tObject.visible = false;\n        tScene.add(tObject);\n        \n        log.innerText = \"Activando seguimiento...\";\n        \n        // ========== CONFIGURAR MEDIAPIPE ==========\n        const faceMesh = new FaceMesh({\n            locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`\n        });\n        \n        faceMesh.setOptions({\n            maxNumFaces: 1,\n            refineLandmarks: true,\n            minDetectionConfidence: 0.5,\n            minTrackingConfidence: 0.5\n        });\n        \n        faceMesh.onResults((results) => {\n            if (window.cameraManager.currentFilter !== 'sphere_ar' || !tRenderer) return;\n            \n            tRenderer.clear();\n            \n            if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {\n                const landmarks = results.multiFaceLandmarks[0];\n                const nose = landmarks[4]; // Punta de la nariz\n                \n                const x = (nose.x - 0.5) * 640;\n                const y = -(nose.y - 0.5) * 480;\n                const z = -nose.z * 200;\n                \n                tObject.position.set(x, y, z);\n                tObject.rotation.x += 0.01;\n                tObject.rotation.y += 0.02;\n                \n                tObject.visible = true;\n                log.innerText = \"Esfera activa siguiendo nariz\";\n            } else {\n                tObject.visible = false;\n                log.innerText = \"Rostro no detectado\";\n            }\n            \n            tRenderer.render(tScene, tCamera);\n        });\n        \n        // ========== INICIAR CÁMARA ==========\n        tCameraInstance = new Camera(tVideoElement, {\n            onFrame: async () => {\n                if (window.cameraManager.currentFilter === 'sphere_ar') {\n                    await faceMesh.send({image: tVideoElement});\n                }\n            },\n            width: 640,\n            height: 480\n        });\n        \n        await tCameraInstance.start();\n        \n    } catch (e) {\n        log.innerText = \"Error: \" + e.message;\n        console.error(e);\n        detenerSphereAR();\n    }\n}\n</script>",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<div style=\"text-align: center; background: #1a1a1a; padding: 20px; border-radius: 15px; color: white; font-family: sans-serif;\">\n    <h3>Three AR (Esfera Guía)</h3>\n    <p style=\"font-size: 12px; color: #aaa;\">La esfera te esta siguiendo</p>\n\n    <div id=\"threejs_container_sphere\" style=\"position: relative; display: inline-block; width: 640px; height: 480px; border: 2px solid #555; border-radius: 10px; overflow: hidden; background: #000;\">\n        </div>\n\n    <div style=\"margin-top: 15px;\">\n        <button id=\"btn_on_sphere\" onclick=\"iniciarSphereAR()\" style=\"padding: 10px 20px; background: #f1c40f; color: #000; border: none; border-radius: 5px; cursor: pointer; font-weight: bold;\">ACTIVAR ESFERA</button>\n        <button id=\"btn_off_sphere\" onclick=\"detenerSphereAR()\" style=\"padding: 10px 20px; background: #e74c3c; color: white; border: none; border-radius: 5px; cursor: pointer; margin-left: 10px; display: none;\">APAGAR</button>\n    </div>\n\n\n    <div style=\"margin-top: 10px;\">\n        <button id=\"toggle_bg_sphere\" onclick=\"toggleBackgroundSphere()\" style=\"padding: 8px 15px; background: #34495e; color: white; border: none; border-radius: 5px; cursor: pointer; font-size: 12px;\">Activar camara</button>\n    </div>\n\n    <p id=\"log_sphere\" style=\"color: #f1c40f; font-size: 13px; margin-top: 10px;\">Estado: Esperando cámara...</p>\n</div>\n\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js\" crossorigin=\"anonymous\"></script>\n\n<script>\nlet tScene, tCamera, tRenderer, tObject;\nlet tCameraInstance = null;\nlet tVideoElement = null;\nlet isVideoVisible = false; // Estado inicial: Video oculto\n\n// --- FUNCIÓN CORREGIDA PARA MOSTRAR/OCULTAR VIDEO ---\nfunction toggleBackgroundSphere() {\n    // 1. Buscamos el elemento de video directamente por su ID para asegurar que existe\n    const vid = document.getElementById('v_threejs_sphere');\n    const btn = document.getElementById('toggle_bg_sphere');\n\n    if (vid) {\n        isVideoVisible = !isVideoVisible;\n        // Cambiamos el estilo display entre 'block' (visible) y 'none' (oculto)\n        vid.style.display = isVideoVisible ? 'block' : 'none';\n        // Actualizamos el texto del botón\n        if (btn) btn.innerText = isVideoVisible ? 'Ocultar Mi Cara' : 'Mostrar Mi Cara';\n    } else {\n        console.log(\"El video aún no está listo. Inicia la cámara primero.\");\n    }\n}\n\n// Función para crear la esfera amarilla de alambre (igual que tu imagen)\nfunction crearEsferaGuia() {\n    const geometry = new THREE.IcosahedronGeometry(30, 2); // Esfera geodésica\n    const material = new THREE.MeshBasicMaterial({ \n        color: 0xffff00,       // Amarillo\n        wireframe: true        // Modo alambre\n    });\n    return new THREE.Mesh(geometry, material);\n}\n\n// Limpieza de memoria\nfunction detenerSphereAR() {\n    if (tCameraInstance) {\n        try { tCameraInstance.stop(); } catch(e) { console.log(e); }\n        tCameraInstance = null;\n    }\n\n    if (tRenderer) { tRenderer.dispose(); tRenderer = null; }\n    if (tScene) {\n        while(tScene.children.length > 0) { tScene.remove(tScene.children[0]); }\n        tScene = null;\n    }\n\n    if (window.cameraManager && window.cameraManager.currentStream) {\n        window.cameraManager.currentStream.getTracks().forEach(t => t.stop());\n        window.cameraManager.currentStream = null;\n    }\n\n    if (tVideoElement) {\n        if (tVideoElement.srcObject) {\n            tVideoElement.srcObject.getTracks().forEach(t => t.stop());\n            tVideoElement.srcObject = null;\n        }\n        tVideoElement = null;\n    }\n\n    // Reseteamos estado del botón\n    isVideoVisible = false;\n    const btn = document.getElementById('toggle_bg_sphere');\n    if(btn) btn.innerText = 'Mostrar Mi Cara';\n\n    const container = document.getElementById('threejs_container_sphere');\n    if (container) container.innerHTML = '';\n\n    document.getElementById('btn_on_sphere').style.display = 'inline-block';\n    document.getElementById('btn_off_sphere').style.display = 'none';\n    document.getElementById('log_sphere').innerText = \"Sistema apagado.\";\n}\n\nasync function iniciarSphereAR() {\n    const log = document.getElementById('log_sphere');\n\n    try {\n        log.innerText = \"Iniciando sistema...\";\n        detenerSphereAR(); // Limpieza previa\n        await new Promise(resolve => setTimeout(resolve, 300));\n\n        const container = document.getElementById('threejs_container_sphere');\n\n        // 1. CREAMOS EL VIDEO (Oculto por defecto con display: none)\n        tVideoElement = document.createElement('video');\n        tVideoElement.id = 'v_threejs_sphere'; // ID importante para el botón\n        tVideoElement.width = 640;\n        tVideoElement.height = 480;\n        tVideoElement.autoplay = true;\n        tVideoElement.playsInline = true;\n        tVideoElement.muted = true;\n        // style.display = none inicial\n        tVideoElement.style.cssText = 'position: absolute; top: 0; left: 0; transform: scaleX(-1); width: 100%; height: 100%; display: none;';\n        container.appendChild(tVideoElement);\n\n        // 2. CREAMOS EL CANVAS (Donde se dibuja la esfera)\n        const tCanvas = document.createElement('canvas');\n        tCanvas.id = 'c_threejs_sphere';\n        tCanvas.width = 640;\n        tCanvas.height = 480;\n        // z-index alto para estar encima\n        tCanvas.style.cssText = 'position: absolute; top: 0; left: 0; transform: scaleX(-1); width: 100%; height: 100%; z-index: 10;';\n        container.appendChild(tCanvas);\n\n        log.innerText = \"Accediendo a cámara...\";\n\n        const stream = await navigator.mediaDevices.getUserMedia({ \n            video: { width: 640, height: 480, facingMode: 'user' } \n        });\n\n        if (!window.cameraManager) window.cameraManager = {};\n        window.cameraManager.currentStream = stream;\n        window.cameraManager.currentFilter = 'sphere_ar';\n\n        tVideoElement.srcObject = stream;\n        await tVideoElement.play();\n\n        await new Promise(resolve => {\n            if (tVideoElement.videoWidth > 0) resolve();\n            else tVideoElement.onloadedmetadata = () => resolve();\n        });\n\n        document.getElementById('btn_on_sphere').style.display = 'none';\n        document.getElementById('btn_off_sphere').style.display = 'inline-block';\n\n        log.innerText = \"Configurando 3D...\";\n\n        // ========== CONFIGURAR THREE.JS ==========\n        tScene = new THREE.Scene();\n        tCamera = new THREE.OrthographicCamera(-320, 320, 240, -240, 0.1, 1000);\n        tCamera.position.z = 10;\n\n        tRenderer = new THREE.WebGLRenderer({ \n            canvas: tCanvas, \n            alpha: true, // Fondo transparente\n            antialias: true \n        });\n        tRenderer.setSize(640, 480);\n\n        // Agregar la Esfera Amarilla\n        tObject = crearEsferaGuia();\n        tObject.visible = false;\n        tScene.add(tObject);\n\n        log.innerText = \"Activando seguimiento...\";\n\n        // ========== CONFIGURAR MEDIAPIPE ==========\n        const faceMesh = new FaceMesh({\n            locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`\n        });\n\n        faceMesh.setOptions({\n            maxNumFaces: 1,\n            refineLandmarks: true,\n            minDetectionConfidence: 0.5,\n            minTrackingConfidence: 0.5\n        });\n\n        faceMesh.onResults((results) => {\n            if (window.cameraManager.currentFilter !== 'sphere_ar' || !tRenderer) return;\n\n            tRenderer.clear();\n\n            if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {\n                const landmarks = results.multiFaceLandmarks[0];\n                const nose = landmarks[4]; // Punta de la nariz\n\n                const x = (nose.x - 0.5) * 640;\n                const y = -(nose.y - 0.5) * 480;\n                const z = -nose.z * 200;\n\n                tObject.position.set(x, y, z);\n                tObject.rotation.x += 0.01;\n                tObject.rotation.y += 0.02;\n\n                tObject.visible = true;\n                log.innerText = \"Esfera activa siguiendo nariz\";\n            } else {\n                tObject.visible = false;\n                log.innerText = \"Rostro no detectado\";\n            }\n\n            tRenderer.render(tScene, tCamera);\n        });\n\n        // ========== INICIAR CÁMARA ==========\n        tCameraInstance = new Camera(tVideoElement, {\n            onFrame: async () => {\n                if (window.cameraManager.currentFilter === 'sphere_ar') {\n                    await faceMesh.send({image: tVideoElement});\n                }\n            },\n            width: 640,\n            height: 480\n        });\n\n        await tCameraInstance.start();\n\n    } catch (e) {\n        log.innerText = \"Error: \" + e.message;\n        console.error(e);\n        detenerSphereAR();\n    }\n}\n</script>\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5
    }
  ]
}